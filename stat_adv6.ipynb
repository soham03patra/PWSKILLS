{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2d3a334-d644-4c20-96aa-67e0f092f7c4",
   "metadata": {},
   "source": [
    "## Q(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a8a4d0-344e-438f-a0fe-765061fd9c13",
   "metadata": {},
   "source": [
    " **Here are the assumptions required for ANOVA, along with examples of violations and their potential impacts:**\n",
    "\n",
    "**1. Normality of Distributions:**\n",
    "\n",
    "- **Assumption:** The dependent variable within each group should be approximately normally distributed.\n",
    "- **Violation:** Skewed distributions, outliers, or heavy tails.\n",
    "- **Impact:** Increased risk of Type I errors (false positives), especially with smaller sample sizes. Results may not accurately reflect the true relationships between groups.\n",
    "\n",
    "**2. Homogeneity of Variances (Homoscedasticity):**\n",
    "\n",
    "- **Assumption:** The variances of the dependent variable should be equal across all groups.\n",
    "- **Violation:** Unequal variances (heteroscedasticity).\n",
    "- **Impact:** F-test results become less reliable, affecting the accuracy of p-values and conclusions about group differences.\n",
    "\n",
    "**3. Independence of Observations:**\n",
    "\n",
    "- **Assumption:** Observations within and between groups should be independent of each other.\n",
    "- **Violation:** Repeated measures on the same subjects, clustered data, or hierarchical structures.\n",
    "- **Impact:** Inflated Type I error rates, as the model assumes unrelated observations.\n",
    "\n",
    "**4. Interval or Ratio Dependent Variable:**\n",
    "\n",
    "- **Assumption:** The dependent variable must be continuous, measured on an interval or ratio scale.\n",
    "- **Violation:** Ordinal or nominal dependent variables.\n",
    "- **Impact:** ANOVA is not appropriate for non-continuous variables, and alternative methods like non-parametric tests are needed.\n",
    "\n",
    "**Additional Considerations:**\n",
    "\n",
    "- **Additive Factor Effects:** ANOVA assumes that the effects of independent variables on the dependent variable are additive, meaning they combine in a linear fashion.\n",
    "\n",
    "**Addressing Violations:**\n",
    "\n",
    "- **Transformations:** Normalize skewed distributions or stabilize variances.\n",
    "- **Non-Parametric Tests:** Use tests like Kruskal-Wallis or Friedman's ANOVA for non-normal data or unequal variances.\n",
    "- **Robust ANOVA Methods:** Employ techniques less sensitive to violations, such as Welch's ANOVA for unequal variances.\n",
    "- **Mixed Models:** Account for non-independence in repeated measures or hierarchical designs.\n",
    "\n",
    "**It's crucial to check these assumptions before conducting ANOVA and carefully consider appropriate remedies if violations occur to ensure the validity and trustworthiness of the results.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ee56cb-b87e-4798-a82e-389b36698542",
   "metadata": {},
   "source": [
    "## Q(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00171675-0508-48c6-899d-1eebfdec0b51",
   "metadata": {},
   "source": [
    "There are three main types of ANOVA (Analysis of Variance) based on the number of independent variables used in your analysis:\n",
    "\n",
    "1. One-Way ANOVA:\n",
    "\n",
    "Situation: Compare the means of three or more groups on a single dependent variable.\n",
    "Examples: Studying the effect of different fertilizer types on plant growth, comparing average student scores in different teaching methods, or analyzing protein levels in three diet groups.\n",
    "\n",
    "2. Two-Way ANOVA:\n",
    "\n",
    "Situation: Investigate the effects of two independent variables on a single dependent variable, potentially including their interaction effect.\n",
    "Examples: Examining the combined effect of exercise frequency and intensity on weight loss, analyzing the impact of both fertilizer type and soil acidity on crop yield, or studying the influence of both gender and age on job satisfaction.\n",
    "\n",
    "3. N-Way ANOVA (Multiple-way ANOVA):\n",
    "\n",
    "Situation: Analyze the effects of three or more independent variables and their interactions on a single dependent variable.\n",
    "Examples: Exploring the combined influence of fertilizer type, water stress, and temperature on fruit quality, researching the effects of medication, dosage, and treatment duration on symptom reduction, or investigating the impact of teacher experience, class size, and student socioeconomic status on academic performance.\n",
    "Choosing the right type of ANOVA depends on your research question and the number of independent variables you want to investigate.\n",
    "\n",
    "Here are some additional points to consider:\n",
    "\n",
    "One-way ANOVA is the simplest form and suitable for basic comparisons between groups.\n",
    "Two-way ANOVA allows for a more nuanced analysis by revealing potential interactions between independent variables.\n",
    "N-way ANOVA is useful for complex research designs with multiple factors influencing the outcome, but it also requires larger data sets and more elaborate interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62121f68-04dd-4cee-bce6-2e166c9d3b25",
   "metadata": {},
   "source": [
    "## Q(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e042ff51-489f-47af-acbb-ce36499f7dc6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Concept:**\n",
    "\n",
    "- **Partitioning of variance** is a fundamental concept in ANOVA that involves dividing the total variance in a dataset into different components to determine whether the observed differences between groups are statistically significant.\n",
    "\n",
    "**Process:**\n",
    "\n",
    "1. **Total Sum of Squares (SST):** Represents the total variability of all scores around the grand mean (mean of all scores).\n",
    "2. **Between-Groups Sum of Squares (SSB):** Measures the variability attributed to differences between the group means.\n",
    "3. **Within-Groups Sum of Squares (SSW):** Captures the variability within each group, often considered as \"error\" or \"unexplained\" variance.\n",
    "\n",
    "**Relationship:**\n",
    "\n",
    "- SST = SSB + SSW\n",
    "\n",
    "**Importance:**\n",
    "\n",
    "1. **F-Test:** ANOVA uses the F-test to compare the ratio of SSB to SSW. A large F-value indicates that the differences between group means are substantial relative to the variability within groups, suggesting a significant effect of the independent variable.\n",
    "2. **Understanding Sources of Variation:** Partitioning variance helps identify whether differences in scores are due to the independent variable (group membership) or random error.\n",
    "3. **Statistical Power:** By isolating the variability caused by the independent variable, ANOVA increases the ability to detect significant effects, even with smaller sample sizes.\n",
    "4. **Effect Size Calculations:** Partitioned variance components are used to calculate effect sizes like eta-squared, providing a standardized measure of the strength of the relationship between variables.\n",
    "5. **Model Assumptions:** Analyzing the partitioned variance can reveal violations of assumptions like homogeneity of variances, crucial for interpreting ANOVA results correctly.\n",
    "\n",
    "**In essence, understanding partitioning of variance is essential for:**\n",
    "\n",
    "- **Assessing the significance of group differences in ANOVA.**\n",
    "- **Interpreting ANOVA results meaningfully.**\n",
    "- **Evaluating statistical power and effect sizes.**\n",
    "- **Diagnosing potential violations of model assumptions.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c987bae8-0316-4f14-bf8b-77b5a53edf9f",
   "metadata": {},
   "source": [
    "## Q(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cfbc492-5e3d-4c6b-bb48-386a9de6cece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sum of Squares (SST): 1003.6000\n",
      "Explained Sum of Squares (SSE): 867.6000\n",
      "Residual Sum of Squares (SSR): 136.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "group_a = [75, 80, 85, 78, 82]\n",
    "group_b = [68, 72, 65, 74, 70]\n",
    "group_c = [88, 85, 90, 92, 87]\n",
    "\n",
    "# Combine all groups into a single list\n",
    "all_data = np.concatenate([group_a, group_b, group_c])\n",
    "\n",
    "# Calculate overall mean (XÌ„total)\n",
    "overall_mean = np.mean(all_data)\n",
    "\n",
    "# Calculate Total Sum of Squares (SST)\n",
    "sst = np.sum((all_data - overall_mean)**2)\n",
    "\n",
    "# Calculate group means\n",
    "mean_a = np.mean(group_a)\n",
    "mean_b = np.mean(group_b)\n",
    "mean_c = np.mean(group_c)\n",
    "\n",
    "# Calculate Explained Sum of Squares (SSE)\n",
    "sse = len(group_a) * (mean_a - overall_mean)**2 + len(group_b) * (mean_b - overall_mean)**2 + len(group_c) * (mean_c - overall_mean)**2\n",
    "\n",
    "# Calculate Residual Sum of Squares (SSR)\n",
    "ssr = np.sum((group_a - mean_a)**2) + np.sum((group_b - mean_b)**2) + np.sum((group_c - mean_c)**2)\n",
    "\n",
    "# Verify that SST equals the sum of SSE and SSR\n",
    "assert np.isclose(sst, sse + ssr)\n",
    "\n",
    "print(f\"Total Sum of Squares (SST): {sst:.4f}\")\n",
    "print(f\"Explained Sum of Squares (SSE): {sse:.4f}\")\n",
    "print(f\"Residual Sum of Squares (SSR): {ssr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa22270-06dc-4e01-8ada-80f81deece34",
   "metadata": {},
   "source": [
    "## Q(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8625c35f-52f2-4f5b-8c47-e97cccfd05b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Effect of Factor A: 39.4167\n",
      "Main Effect of Factor B: 156.2500\n",
      "Interaction Effect: 5.7500\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "data = {\n",
    "    'Y': [10, 12, 14, 15, 8, 11, 13, 16, 9, 12, 18, 20, 16, 14, 22, 24],\n",
    "    'FactorA': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'C', 'C', 'C', 'C', 'D', 'D', 'D', 'D'],\n",
    "    'FactorB': ['X', 'X', 'Y', 'Y', 'X', 'X', 'Y', 'Y', 'X', 'X', 'Y', 'Y', 'X', 'X', 'Y', 'Y']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "formula = 'Y ~ FactorA + FactorB + FactorA:FactorB'\n",
    "model = ols(formula, data=df).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "main_effect_A = anova_table['sum_sq']['FactorA'] / anova_table['df']['FactorA']\n",
    "main_effect_B = anova_table['sum_sq']['FactorB'] / anova_table['df']['FactorB']\n",
    "interaction_effect = anova_table['sum_sq']['FactorA:FactorB'] / anova_table['df']['FactorA:FactorB']\n",
    "\n",
    "print(f\"Main Effect of Factor A: {main_effect_A:.4f}\")\n",
    "print(f\"Main Effect of Factor B: {main_effect_B:.4f}\")\n",
    "print(f\"Interaction Effect: {interaction_effect:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab82aa48-66a9-4fa9-bd03-3ba8efef44f7",
   "metadata": {},
   "source": [
    "## Q(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f406c3e-fd96-43d3-9d8c-33855a04f187",
   "metadata": {},
   "source": [
    "Significant Difference:\n",
    "\n",
    "The obtained p-value of 0.02 is less than the conventional significance level of 0.05. This means we have enough evidence to reject the null hypothesis that all group means are equal.\n",
    "We can conclude that there is a statistically significant difference between at least two of the group means.\n",
    "\n",
    "2. Strength of Evidence:\n",
    "\n",
    "The F-statistic of 5.23 indicates a moderate to strong effect size. It suggests that the variability between the group means is substantial relative to the variability within the groups.\n",
    "\n",
    "3. Additional Information:\n",
    "\n",
    "ANOVA does not specify which particular groups differ significantly from each other.\n",
    "To identify the specific group differences, post-hoc tests such as Tukey's HSD or pairwise t-tests would be necessary.\n",
    "\n",
    "4. Assumptions:\n",
    "\n",
    "Before fully accepting these conclusions, it's essential to ensure that the assumptions of ANOVA (normality, homogeneity of variances, and independence of observations) have been met. Violations of these assumptions could impact the validity of the results.\n",
    "\n",
    "5. Practical Significance:\n",
    "\n",
    "While statistical significance is important, consider the practical significance of the differences as well. Assess the effect size using measures like eta-squared or Cohen's d to determine the practical relevance of the findings.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "The results suggest that the independent variable (grouping factor) has a significant effect on the dependent variable.\n",
    "There are meaningful differences in the dependent variable's values across different groups.\n",
    "Further investigation using post-hoc tests can pinpoint the exact pairs of groups that exhibit significant differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f17eb89-bad8-4332-b2ca-8992ddefc1e0",
   "metadata": {},
   "source": [
    "## Q(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e456aeef-8654-43b2-bf6c-05520711a93a",
   "metadata": {},
   "source": [
    "**Here's how to handle missing data in repeated measures ANOVA, along with the potential consequences of different methods:**\n",
    "\n",
    "**Methods for Handling Missing Data:**\n",
    "\n",
    "1. **Complete Case Analysis (Listwise Deletion):**\n",
    "   - Removes any participant with missing data for any time point.\n",
    "   - Consequences:\n",
    "     - Can lead to substantial loss of data and reduced power, especially with large missingness.\n",
    "     - May introduce bias if missingness is not random.\n",
    "\n",
    "2. **Pairwise Deletion:**\n",
    "   - Uses all available data for each comparison, excluding only participants with missing data for the specific time points being compared.\n",
    "   - Consequences:\n",
    "     - Can lead to different sample sizes for different comparisons, complicating interpretation.\n",
    "     - May still introduce bias if missingness is not random.\n",
    "\n",
    "3. **Mean Imputation:**\n",
    "   - Replaces missing values with the mean of the observed scores for that time point.\n",
    "   - Consequences:\n",
    "     - Can underestimate variability and distort relationships between variables.\n",
    "     - May not be appropriate if missingness patterns are complex.\n",
    "\n",
    "4. **Last Observation Carried Forward (LOCF):**\n",
    "   - Imputes missing values with the last observed non-missing value for that participant.\n",
    "   - Consequences:\n",
    "     - Assumes no change over time, which may not be realistic.\n",
    "     - Can lead to biased results if missingness is related to change over time.\n",
    "\n",
    "5. **Mixed Models (Mixed Effects Models):**\n",
    "   - A sophisticated statistical approach that can handle missing data more flexibly by incorporating all available data and modeling the covariance structure of the repeated measures.\n",
    "   - Consequences:\n",
    "     - Requires more complex statistical expertise and software.\n",
    "     - Assumptions about the missing data mechanism need to be carefully considered.\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- **Prevention:** Design studies to minimize missing data, ensuring complete and accurate data collection.\n",
    "- **Mechanism:** Understand the reasons for missingness to choose appropriate methods.\n",
    "- **Multiple Imputation:** Consider multiple imputation techniques, which create multiple plausible datasets with imputed values and combine results for more robust inferences.\n",
    "- **Mixed Models:** Explore mixed models for their flexibility in handling missing data and accounting for correlations between repeated measures.\n",
    "\n",
    "**Key Considerations:**\n",
    "\n",
    "- **Assumptions:** Each method has assumptions about the missing data mechanism (random or non-random) that should be assessed.\n",
    "- **Bias:** Inappropriate methods can lead to biased results and incorrect conclusions.\n",
    "- **Power:** Missing data can reduce statistical power, making it harder to detect significant effects.\n",
    "- **Interpretation:** Carefully consider the impact of missing data handling on the interpretation of results.\n",
    "\n",
    "**Consult with a statistician for guidance on the most appropriate method based on the specific study design and missing data patterns.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bd4ab7-4e1a-4eed-87dd-85e450c3c401",
   "metadata": {},
   "source": [
    "## Q(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f64d392-ee80-4de4-ad5d-82a9c3cdc0d7",
   "metadata": {},
   "source": [
    "**Here are some common post-hoc tests used after ANOVA, along with their uses and an example:**\n",
    "\n",
    "**1. Tukey's Honest Significant Difference (HSD):**\n",
    "\n",
    "- **When to use:** For comparing all possible pairs of means in a balanced design (equal sample sizes in each group).\n",
    "- **Features:** Controls the family-wise error rate (FWER), ensuring the overall probability of making at least one Type I error among all comparisons remains at the desired level (usually 0.05).\n",
    "- **Example:** Comparing the effects of four different fertilizers on plant growth. If ANOVA indicates a significant difference, Tukey's HSD would determine which specific fertilizers lead to significantly different growth rates.\n",
    "\n",
    "**2. Bonferroni Correction:**\n",
    "\n",
    "- **When to use:** For pairwise comparisons with unequal sample sizes or when a more conservative approach is desired.\n",
    "- **Features:** Adjusts the significance level for each individual comparison to control the FWER.\n",
    "- **Example:** Comparing the effectiveness of two teaching methods on student test scores, but with different class sizes in each method. Bonferroni correction would account for the unequal sample sizes in the comparisons.\n",
    "\n",
    "**3. ScheffÃ©'s Test:**\n",
    "\n",
    "- **When to use:** For more complex comparisons beyond pairwise, such as comparing combinations of means or linear combinations of means.\n",
    "- **Features:** Very conservative, controls the FWER for any possible comparison.\n",
    "- **Example:** Comparing the effects of two drugs and a placebo on blood pressure, where you might want to compare the average effect of the drugs to the placebo, or compare the difference between the two drugs.\n",
    "\n",
    "**4. Dunnett's Test:**\n",
    "\n",
    "- **When to use:** For comparing multiple treatment groups to a single control group.\n",
    "- **Features:** More powerful than Bonferroni or ScheffÃ© when only comparing to a control.\n",
    "- **Example:** Comparing the effectiveness of three new medications to a standard treatment for a disease. Dunnett's test would focus on identifying treatments that are significantly better than the control.\n",
    "\n",
    "**5. Games-Howell Test:**\n",
    "\n",
    "- **When to use:** For pairwise comparisons when variances are unequal (heteroscedasticity).\n",
    "- **Features:** Robust to violations of homogeneity of variance assumption.\n",
    "- **Example:** Comparing the salaries of employees in different job categories with varying levels of experience, where salary distributions might have different variances.\n",
    "\n",
    "**Remember:**\n",
    "\n",
    "- Post-hoc tests are only conducted if the overall ANOVA test is significant, indicating at least one difference among the means.\n",
    "- The choice of post-hoc test depends on the specific research question, design, and assumptions of the data.\n",
    "- It's essential to consider both statistical significance and practical significance when interpreting the results of post-hoc tests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb7da2f-ac50-4bf1-8ede-d0840e172397",
   "metadata": {},
   "source": [
    "## Q(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0657c7b-dacc-46d0-855f-cb7564b45346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 67.6185\n",
      "P-value: 0.0000\n",
      "There is significant evidence to reject the null hypothesis.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Example data\n",
    "np.random.seed(42)  # for reproducibility\n",
    "weight_loss_A = np.random.normal(5, 1, 50)  # mean=5, std=1\n",
    "weight_loss_B = np.random.normal(6, 1, 50)  # mean=6, std=1\n",
    "weight_loss_C = np.random.normal(7, 1, 50)  # mean=7, std=1\n",
    "\n",
    "# Combine data\n",
    "weight_loss_data = np.concatenate([weight_loss_A, weight_loss_B, weight_loss_C])\n",
    "\n",
    "# Create group labels\n",
    "groups = ['A'] * 50 + ['B'] * 50 + ['C'] * 50\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = f_oneway(weight_loss_A, weight_loss_B, weight_loss_C)\n",
    "\n",
    "# Report results\n",
    "print(f\"F-statistic: {f_statistic:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "# Interpret the results\n",
    "if p_value < 0.05:\n",
    "    print(\"There is significant evidence to reject the null hypothesis.\")\n",
    "else:\n",
    "    print(\"There is not enough evidence to reject the null hypothesis.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151504b2-1f8b-4c56-b962-bbdfa06930ec",
   "metadata": {},
   "source": [
    "## Q(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b49581bd-c347-4e5f-b090-26833ff4efd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        sum_sq    df         F    PR(>F)\n",
      "Program               2.514772   2.0  0.344485  0.709581\n",
      "Experience            0.479063   1.0  0.131248  0.718051\n",
      "Program:Experience    1.592393   2.0  0.218133  0.804472\n",
      "Residual            306.603758  84.0       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Example data\n",
    "np.random.seed(42)  # for reproducibility\n",
    "\n",
    "# Simulating data with two factors (Software Programs and Experience Level)\n",
    "data = {\n",
    "    'Time': np.random.normal(loc=10, scale=2, size=90),\n",
    "    'Program': np.repeat(['A', 'B', 'C'], 30),\n",
    "    'Experience': np.tile(['Novice', 'Experienced'], 45),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Fit two-way ANOVA model\n",
    "formula = 'Time ~ Program + Experience + Program:Experience'\n",
    "model = ols(formula, data=df).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Report results\n",
    "print(anova_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c7136d-aac7-4977-bac9-1138a65d5bd6",
   "metadata": {},
   "source": [
    "## Q(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "311ebe0b-299c-4672-b4e1-3925148905e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-statistic: -4.7547\n",
      "P-value: 0.0000\n",
      "There is a significant difference in test scores between the two groups.\n",
      "  Multiple Comparison of Means - Tukey HSD, FWER=0.05   \n",
      "========================================================\n",
      " group1    group2    meandiff p-adj lower  upper  reject\n",
      "--------------------------------------------------------\n",
      "Control Experimental   6.2615   0.0 3.6645 8.8585   True\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Simulated data\n",
    "np.random.seed(42)  # for reproducibility\n",
    "control_group = np.random.normal(70, 10, 100)  # mean=70, std=10\n",
    "experimental_group = np.random.normal(75, 10, 100)  # mean=75, std=10\n",
    "\n",
    "# Perform two-sample t-test\n",
    "t_statistic, p_value = ttest_ind(control_group, experimental_group)\n",
    "\n",
    "# Report results of the t-test\n",
    "print(f\"T-statistic: {t_statistic:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "# Check for significance\n",
    "if p_value < 0.05:\n",
    "    print(\"There is a significant difference in test scores between the two groups.\")\n",
    "else:\n",
    "    print(\"There is no significant difference in test scores between the two groups.\")\n",
    "\n",
    "# Follow up with a post-hoc test (Tukey's HSD) if the results are significant\n",
    "if p_value < 0.05:\n",
    "    # Combine data and labels\n",
    "    all_scores = np.concatenate([control_group, experimental_group])\n",
    "    group_labels = ['Control'] * 100 + ['Experimental'] * 100\n",
    "\n",
    "    # Perform post-hoc test\n",
    "    results = pairwise_tukeyhsd(all_scores, group_labels)\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef70ae4f-8e4b-4953-bdc2-d037f9c77463",
   "metadata": {},
   "source": [
    "## Q(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5f02c09-b376-4a6d-84f5-44bb04fa6d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Anova\n",
      "===================================\n",
      "      F Value Num DF  Den DF Pr > F\n",
      "-----------------------------------\n",
      "Store  0.1808 2.0000 58.0000 0.8350\n",
      "===================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Simulated data\n",
    "np.random.seed(42)  # for reproducibility\n",
    "\n",
    "# Generate random sales data for three stores across 30 days\n",
    "data = {\n",
    "    'Day': np.repeat(range(1, 31), 3),\n",
    "    'Store': np.tile(['Store A', 'Store B', 'Store C'], 30),\n",
    "    'Sales': np.random.normal(loc=100, scale=20, size=90),  # mean=100, std=20\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert the 'Day' column to categorical\n",
    "df['Day'] = pd.Categorical(df['Day'])\n",
    "\n",
    "# Fit repeated measures ANOVA model\n",
    "rm_anova = AnovaRM(df, 'Sales', 'Day', within=['Store'])\n",
    "results = rm_anova.fit()\n",
    "\n",
    "# Report results of repeated measures ANOVA\n",
    "print(results)\n",
    "\n",
    "# Follow up with post-hoc test (Tukey's HSD) if the results are significant\n",
    "if results.anova_table['Pr > F']['Store'] < 0.05:\n",
    "    posthoc = pairwise_tukeyhsd(df['Sales'], df['Store'])\n",
    "    print(posthoc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d24503f-44ff-4062-b0cf-d7e07e399038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d29b545-8c4d-4143-906b-22690b04634d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

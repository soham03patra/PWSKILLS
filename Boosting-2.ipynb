{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87aacc3c-0fb1-46ba-b50a-3aaf5c7b295b",
   "metadata": {},
   "source": [
    "# Q1. What is Gradient Boosting Regression?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7904eec-1c31-4673-91be-ada5cb09a4e2",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique used for regression tasks. It is an ensemble learning method that combines the predictions of multiple weak learners (typically decision trees) to create a strong learner. The key idea behind Gradient Boosting Regression is to sequentially train a series of regression trees, each one trained to correct the errors of the previous trees.\n",
    "\n",
    "Here's a high-level overview of how Gradient Boosting Regression works:\n",
    "\n",
    "1. **Initialize the Model:** The initial model is set as a simple regression model, usually the mean of the target variable.\n",
    "\n",
    "2. **Fit a Tree:** A regression tree is fit to the residuals (the differences between the actual and predicted values) of the current model. This tree is trained to predict the residuals, which are the errors made by the current model.\n",
    "\n",
    "3. **Update the Model:** The predictions of the new tree are added to the predictions of the current model, updating the model's predictions.\n",
    "\n",
    "4. **Repeat:** Steps 2 and 3 are repeated for a specified number of iterations (or until a stopping criterion is met). Each new tree is trained to predict the residuals of the current model, with the model's predictions being updated after each tree is fit.\n",
    "\n",
    "5. **Final Prediction:** The final prediction is the sum of the predictions of all the trees in the ensemble.\n",
    "\n",
    "Gradient Boosting Regression is known for its ability to handle complex relationships in the data and its resistance to overfitting. However, it can be computationally expensive and sensitive to hyperparameters, so care must be taken when tuning the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e424a65a-0ae3-4d23-b206-75b1a1360828",
   "metadata": {},
   "source": [
    "# Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7751c3cc-008e-40b8-989f-e652d9e47b88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.tree_weights = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92aca036-89ce-47e0-a370-655bcbd3a934",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def fit(self, X, y):\n",
    "        # Initialize the model predictions as the mean of the target variable\n",
    "        self.base_prediction = np.mean(y)\n",
    "        y_pred = np.full_like(y, self.base_prediction)\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            # Calculate the residuals\n",
    "            residuals = y - y_pred\n",
    "\n",
    "            # Fit a regression tree to the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Calculate the tree weight using the learning rate\n",
    "            tree_weight = self.learning_rate\n",
    "            self.tree_weights.append(tree_weight)\n",
    "\n",
    "            # Update the model predictions\n",
    "            y_pred += tree_weight * tree.predict(X)\n",
    "            self.trees.append(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b00926cd-a29b-41e6-8cf8-a47d853e3470",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def predict(self, X):\n",
    "        # Initialize predictions as the base prediction\n",
    "        y_pred = np.full(X.shape[0], self.base_prediction)\n",
    "\n",
    "        # Add the predictions of each tree weighted by the tree weight\n",
    "        for tree, tree_weight in zip(self.trees, self.tree_weights):\n",
    "            y_pred += tree_weight * tree.predict(X)\n",
    "\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56a15db4-9487-4e59-902f-249603efdd68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbf5e124-0c15-4b14-abfd-2132526f1c05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate a small regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "325a7367-5c80-41a8-96bd-2a3b58aafcf1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the gradient boosting model\n",
    "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3796ea7e-fe0e-4d1a-af26-9ee736736b26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = gb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "207d69d1-6170-4f19-ab79-e74feacf113b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.3379888778506104\n",
      "R-squared: 0.9990403356176427\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440b4136-e453-4293-b89e-3b45211f44a2",
   "metadata": {},
   "source": [
    "# Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e96522a-5239-4d71-b709-b0cb371c1b2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51d544c4-48df-4096-be00-e28aa88d9c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.tree_weights = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "624d8d7e-da8e-4361-ba5e-171f2fac0e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def fit(self, X, y):\n",
    "        # Initialize the model predictions as the mean of the target variable\n",
    "        self.base_prediction = np.mean(y)\n",
    "        y_pred = np.full_like(y, self.base_prediction)\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            # Calculate the residuals\n",
    "            residuals = y - y_pred\n",
    "\n",
    "            # Fit a regression tree to the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Calculate the tree weight using the learning rate\n",
    "            tree_weight = self.learning_rate\n",
    "            self.tree_weights.append(tree_weight)\n",
    "\n",
    "            # Update the model predictions\n",
    "            y_pred += tree_weight * tree.predict(X)\n",
    "            self.trees.append(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d40bc38-c5e0-4143-8a1f-42ac2e45049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def predict(self, X):\n",
    "        # Initialize predictions as the base prediction\n",
    "        y_pred = np.full(X.shape[0], self.base_prediction)\n",
    "\n",
    "        # Add the predictions of each tree weighted by the tree weight\n",
    "        for tree, tree_weight in zip(self.trees, self.tree_weights):\n",
    "            y_pred += tree_weight * tree.predict(X)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ecd8e806-7b99-4241-bc3e-07acb32a1740",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'max_depth': self.max_depth\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ff4e636-ad32-464e-bfd5-6220709aabdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def set_params(self, **params):\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfa6f9a-0072-403f-9a8e-9fab72ed7e98",
   "metadata": {},
   "source": [
    "# Q4. What is a weak learner in Gradient Boosting?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a925bac8-5bce-456f-94c1-2ba97da93f91",
   "metadata": {},
   "source": [
    "In Gradient Boosting, a weak learner is a simple model that performs slightly better than random guessing on a classification or regression problem. In the context of Gradient Boosting, weak learners are typically decision trees with a shallow depth (e.g., a maximum depth of 1 or 2 for classification problems, and a small maximum depth for regression problems).\n",
    "\n",
    "The key characteristic of a weak learner is that it is only required to perform slightly better than random chance, as the boosting process will combine multiple weak learners to create a strong learner. Each weak learner focuses on a specific subset of the data or a specific aspect of the problem, and by combining the predictions of all the weak learners, the Gradient Boosting algorithm is able to improve the overall performance and generalize well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de7184d-afb3-4001-a25d-190a5bc56353",
   "metadata": {},
   "source": [
    "# Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24265b35-8161-40f5-bdf0-480414747518",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm can be summarized as follows:\n",
    "\n",
    "1. **Sequential Learning:** Gradient Boosting builds an ensemble model (often decision trees) sequentially. Each new model in the sequence corrects the errors made by the previous models. This sequential learning process allows the model to focus on the hard-to-predict examples.\n",
    "\n",
    "2. **Gradient Descent:** The \"Gradient\" in Gradient Boosting refers to the technique of using gradients (derivatives) of the loss function to minimize the loss. In each iteration, the algorithm calculates the gradient of the loss function with respect to the current model's predictions and fits a new model to the residuals (the differences between the actual and predicted values).\n",
    "\n",
    "3. **Gradient Descent in Function Space:** Unlike traditional gradient descent, which updates the parameters of a model, Gradient Boosting updates the function space. Each new model (weak learner) is added to the ensemble to reduce the error of the overall model in the function space.\n",
    "\n",
    "4. **Regularization:** Gradient Boosting includes a regularization parameter to control the complexity of the ensemble. This helps prevent overfitting by penalizing overly complex models.\n",
    "\n",
    "5. **Combining Weak Learners:** By combining multiple weak learners (simple models) into a strong learner (complex model), Gradient Boosting is able to create a highly flexible and powerful model that can capture complex patterns in the data.\n",
    "\n",
    "the intuition behind Gradient Boosting is to iteratively improve the model's predictions by focusing on the errors made by the previous models and combining multiple weak learners to create a strong predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd9546b-2d1c-4d56-b94f-17b029e11f23",
   "metadata": {},
   "source": [
    "# Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed90d4f-be11-49d4-b62b-8188a014d453",
   "metadata": {},
   "source": [
    "Initialize the model: Gradient Boosting starts by initializing the model with a single weak learner, usually a decision tree with just a few levels (shallow tree). This first weak learner makes predictions based on the features of the dataset.\n",
    "\n",
    "Compute the residuals: After making predictions with the current model, the algorithm calculates the residuals, which are the differences between the actual target values and the predicted values. These residuals represent the errors made by the current model.\n",
    "\n",
    "Fit a new weak learner to the residuals: The next step is to fit a new weak learner (usually another decision tree) to the residuals. This new weak learner focuses on learning the patterns in the residuals that the previous model failed to capture.\n",
    "\n",
    "Update the model: The predictions of the new weak learner are combined with the predictions of the previous weak learners to improve the overall model. This combination is done in such a way that the new weak learner corrects the mistakes of the previous ones. The most common approach is to update the model by adding the predictions of the new weak learner multiplied by a small number (learning rate) to the predictions of the previous model.\n",
    "\n",
    "Repeat: Steps 2-4 are repeated iteratively, with each new weak learner focusing on the residuals of the current model. This process continues until a predefined number of weak learners have been added, or until a certain level of performance is achieved.\n",
    "\n",
    "Final prediction: To make a final prediction, the predictions of all the weak learners are combined. Typically, this is done by summing up the predictions of each weak learner, possibly multiplied by a small number to control the contribution of each learner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f0f2fe-fc00-4091-9013-c9d03218cb92",
   "metadata": {},
   "source": [
    "# Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c13391-10d5-4039-9b22-29302d003170",
   "metadata": {},
   "source": [
    "Loss Function Selection: First, you need to select a differentiable loss function that measures the error between the model predictions and the actual target values. Common choices include squared error loss for regression problems and log loss (or cross-entropy loss) for classification problems.\n",
    "\n",
    "Initialize the Model: Initialize the model with a simple prediction, usually the mean (for regression) or the log-odds (for classification) of the target variable.\n",
    "\n",
    "Compute Pseudo-Residuals: For each sample in the dataset, compute the negative gradient of the loss function with respect to the current model's prediction. These are called pseudo-residuals and represent the direction in which the model needs to be corrected.\n",
    "\n",
    "Fit a Weak Learner to the Pseudo-Residuals: Fit a weak learner (often a decision tree) to the pseudo-residuals. This weak learner is trained to predict the residuals left by the current model.\n",
    "\n",
    "Update the Model: Update the current model by adding a fraction of the predictions from the weak learner (scaled by a learning rate parameter). This update aims to correct the errors made by the current model.\n",
    "\n",
    "Repeat Steps 3-5: Repeat steps 3-5 iteratively until a predefined number of weak learners have been added, or until a convergence criterion is met.\n",
    "\n",
    "Final Ensemble Model: Combine the predictions of all weak learners to obtain the final ensemble model. This can be done by summing up the predictions of each weak learner (possibly scaled by a learning rate).\n",
    "\n",
    "Regularization: Optionally, apply regularization techniques to prevent overfitting, such as shrinkage (reducing the contribution of each weak learner) or tree pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab03796-1328-4fd2-967b-a7d60e8f2fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "766e08a9-e573-4d2e-a196-a98d7716578c",
   "metadata": {},
   "source": [
    "# Q1. What is the role of feature selection in anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84775023-754c-459d-9499-2ded41e5a998",
   "metadata": {},
   "source": [
    "1. **Dimensionality Reduction**: Anomaly detection often deals with high-dimensional data, where the number of features is large. Feature selection techniques can reduce the dimensionality of the data by selecting a subset of informative features, which can improve the efficiency and effectiveness of anomaly detection algorithms.\n",
    "\n",
    "2. **Improved Performance**: By focusing on the most informative features, feature selection can enhance the performance of anomaly detection algorithms. It reduces the computational complexity and noise in the data, leading to more accurate anomaly detection results.\n",
    "\n",
    "3. **Reduced Overfitting**: Selecting relevant features helps to mitigate the risk of overfitting, especially in situations where the number of features exceeds the number of observations. By removing redundant or irrelevant features, feature selection reduces the chances of the model capturing noise or spurious correlations in the data.\n",
    "\n",
    "4. **Interpretability**: Feature selection can improve the interpretability of anomaly detection models by identifying the most influential features that contribute to the detection of anomalies. This enables analysts to gain insights into the underlying factors driving anomalous behavior.\n",
    "\n",
    "5. **Faster Training and Inference**: With fewer features to process, anomaly detection models can be trained and deployed more efficiently. Feature selection reduces the computational resources required for both training and inference, making the anomaly detection process more scalable.\n",
    "\n",
    "6. **Robustness**: Selecting relevant features can improve the robustness of anomaly detection models by focusing on the most discriminative aspects of the data. It helps to generalize better to unseen data and ensures that the model's performance is not overly sensitive to noise or irrelevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1744d51-3495-4cb1-bdb1-9a53d409de04",
   "metadata": {},
   "source": [
    "# Q2. What are some common evaluation metrics for anomaly detection algorithms and how are they computed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3630ec-81b7-4a66-94c6-de11c5a6e3de",
   "metadata": {},
   "source": [
    "Several evaluation metrics are commonly used to assess the performance of anomaly detection algorithms. Here are some of the most common ones:\n",
    "\n",
    "1. **True Positive Rate (Sensitivity)**:\n",
    "   - Also known as recall or true positive rate (TPR).\n",
    "   - It measures the proportion of actual anomalies that are correctly identified by the algorithm.\n",
    "   - Computed as the ratio of true positives to the sum of true positives and false negatives.\n",
    "   - Formula: \\( \\text{TPR} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}} \\)\n",
    "\n",
    "2. **True Negative Rate (Specificity)**:\n",
    "   - Also known as specificity or true negative rate (TNR).\n",
    "   - It measures the proportion of non-anomalies that are correctly identified as non-anomalies.\n",
    "   - Computed as the ratio of true negatives to the sum of true negatives and false positives.\n",
    "   - Formula: \\( \\text{TNR} = \\frac{\\text{True Negatives}}{\\text{True Negatives} + \\text{False Positives}} \\)\n",
    "\n",
    "3. **Precision**:\n",
    "   - It measures the proportion of instances identified as anomalies that are actually anomalies.\n",
    "   - Computed as the ratio of true positives to the sum of true positives and false positives.\n",
    "   - Formula: \\( \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}} \\)\n",
    "\n",
    "4. **F1 Score**:\n",
    "   - The harmonic mean of precision and recall (TPR).\n",
    "   - It provides a balance between precision and recall.\n",
    "   - Formula: \\( \\text{F1 Score} = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\)\n",
    "\n",
    "5. **Area Under the Receiver Operating Characteristic (ROC) Curve (AUC-ROC)**:\n",
    "   - It measures the ability of the algorithm to distinguish between anomalies and non-anomalies across various thresholds.\n",
    "   - AUC-ROC ranges from 0 to 1, where a higher value indicates better performance.\n",
    "   - The ROC curve plots the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.\n",
    "\n",
    "6. **Area Under the Precision-Recall Curve (AUC-PR)**:\n",
    "   - Similar to AUC-ROC, but focuses on the precision-recall trade-off.\n",
    "   - It measures the ability of the algorithm to balance precision and recall across different threshold settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d989ac-e98b-48b8-a2ec-4bae734a1a60",
   "metadata": {},
   "source": [
    "# Q3. What is DBSCAN and how does it work for clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8eb811-3f8c-422b-8e2d-6f82705ee482",
   "metadata": {},
   "source": [
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a popular clustering algorithm that groups together data points that are closely packed in high-density regions. Here's how DBSCAN works for clustering:\n",
    "\n",
    "1. **Density-Based Clustering**:\n",
    "   - DBSCAN is a density-based clustering algorithm, which means it identifies clusters based on the density of data points in the feature space rather than assuming a specific number of clusters beforehand.\n",
    "  \n",
    "2. **Core Points, Border Points, and Noise**:\n",
    "   - DBSCAN defines three types of points:\n",
    "     - Core Points: A data point is considered a core point if it has at least a specified number of neighboring points (MinPts) within a specified radius (epsilon).\n",
    "     - Border Points: A data point is considered a border point if it is not a core point but is within the epsilon distance of a core point.\n",
    "     - Noise Points: Data points that are neither core points nor border points are considered noise points or outliers.\n",
    "\n",
    "3. **Cluster Formation**:\n",
    "   - DBSCAN starts by randomly selecting a data point and exploring its neighborhood to identify core points. It expands the cluster by adding neighboring core points and their neighbors recursively until no more core points can be reached.\n",
    "   - Any unvisited data points that are not within the epsilon distance of any core point are labeled as noise points.\n",
    "\n",
    "4. **Parameter Selection**:\n",
    "   - DBSCAN requires two main parameters:\n",
    "     - Epsilon (eps): Specifies the maximum distance between two data points to consider them as neighbors.\n",
    "     - MinPts: Specifies the minimum number of data points within the epsilon distance to consider a point as a core point.\n",
    "\n",
    "5. **Cluster Formation**:\n",
    "   - DBSCAN forms clusters by connecting core points and merging them into larger clusters. Border points may belong to multiple clusters if they are within the epsilon distance of multiple core points.\n",
    "\n",
    "6. **Robustness to Noise and Irregular Cluster Shapes**:\n",
    "   - DBSCAN is robust to noise and capable of identifying clusters of arbitrary shapes. It does not require the number of clusters to be predefined and can handle data with varying cluster densities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb439522-92e8-4d7d-a3ad-c1dd597d0a25",
   "metadata": {},
   "source": [
    "# Q4. How does the epsilon parameter affect the performance of DBSCAN in detecting anomalies?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c382478-5dcc-45a1-a01e-4fc6fc73bca7",
   "metadata": {},
   "source": [
    "The epsilon (ε) parameter in DBSCAN defines the maximum distance between two data points to consider them as neighbors. This parameter plays a critical role in determining the neighborhood size for density estimation, which in turn affects the performance of DBSCAN in detecting anomalies. Here's how the epsilon parameter affects the performance of DBSCAN:\n",
    "\n",
    "1. **Density Sensitivity**:\n",
    "   - Smaller values of epsilon result in tighter clusters, requiring data points to be closer together to be considered neighbors. This increases the sensitivity to local density variations, making DBSCAN more likely to identify anomalies as points with low-density neighborhoods.\n",
    "\n",
    "2. **Anomaly Sensitivity**:\n",
    "   - Larger values of epsilon lead to larger neighborhood sizes, which can merge multiple clusters into a single cluster and reduce the sensitivity to local density variations. As a result, DBSCAN may overlook anomalies located within regions of moderate to high density.\n",
    "\n",
    "3. **Optimal Selection**:\n",
    "   - The optimal value of epsilon depends on the characteristics of the dataset, including the density distribution and the desired sensitivity to anomalies. Selecting an appropriate epsilon value requires careful consideration and may involve experimentation or domain knowledge.\n",
    "\n",
    "4. **Tuning Parameter**:\n",
    "   - The epsilon parameter is a tuning parameter in DBSCAN, meaning that it needs to be carefully chosen based on the specific requirements of the anomaly detection task. Grid search, cross-validation, or other optimization techniques can be used to find the optimal value of epsilon that maximizes anomaly detection performance.\n",
    "\n",
    "5. **Trade-off**:\n",
    "   - There is a trade-off between the sensitivity to anomalies and the risk of including noise or irrelevant data points as anomalies. A smaller epsilon value may detect more anomalies but also increase the likelihood of false positives, while a larger epsilon value may reduce false positives but potentially miss anomalies with low-density neighborhoods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd674a01-ba5c-450b-9820-9afc7700ad92",
   "metadata": {},
   "source": [
    "# Q5. What are the differences between the core, border, and noise points in DBSCAN, and how do they relate to anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b688a2ba-17da-46fe-94bc-084dc7e226d6",
   "metadata": {},
   "source": [
    "In DBSCAN (Density-Based Spatial Clustering of Applications with Noise), data points are classified into three categories: core points, border points, and noise points. Here's an overview of each type and their relation to anomaly detection:\n",
    "\n",
    "1. **Core Points**:\n",
    "   - Core points are data points that have at least a specified number of neighboring points (MinPts) within a specified radius (epsilon, ε).\n",
    "   - They are considered the \"core\" of a cluster and are surrounded by other points within the neighborhood.\n",
    "   - Core points are essential for cluster formation and act as central nodes in densely packed regions of the data.\n",
    "   - From an anomaly detection perspective, core points are less likely to be anomalies because they are surrounded by other points within the cluster.\n",
    "\n",
    "2. **Border Points**:\n",
    "   - Border points are data points that are not core points but are within the epsilon distance of at least one core point.\n",
    "   - They lie on the border of a cluster and are adjacent to one or more core points.\n",
    "   - Border points may belong to multiple clusters if they are within the epsilon distance of multiple core points.\n",
    "   - While border points are part of a cluster, they are less dense than core points and may have fewer neighboring points.\n",
    "   - From an anomaly detection perspective, border points are less likely to be anomalies compared to noise points but may still exhibit anomalous behavior if they are on the outskirts of a cluster.\n",
    "\n",
    "3. **Noise Points (Outliers)**:\n",
    "   - Noise points, also known as outliers, are data points that do not meet the criteria to be classified as core or border points.\n",
    "   - They do not have the minimum number of neighboring points within the epsilon distance and are not within the epsilon distance of any core point.\n",
    "   - Noise points are isolated from dense regions of the data and do not belong to any cluster.\n",
    "   - From an anomaly detection perspective, noise points are more likely to be anomalies because they are not part of any cluster and exhibit behavior that deviates from the majority of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c5725e-e76f-41e7-8c4a-1dfd9dd668a5",
   "metadata": {},
   "source": [
    "# Q6. How does DBSCAN detect anomalies and what are the key parameters involved in the process?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cea623-914d-47cb-aacf-a26bef2fdffb",
   "metadata": {},
   "source": [
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) can detect anomalies indirectly by identifying noise points, which are data points that do not belong to any cluster. Here's how DBSCAN detects anomalies and the key parameters involved in the process:\n",
    "\n",
    "1. **Noise Points (Outliers)**:\n",
    "   - DBSCAN identifies noise points as data points that do not meet the criteria to be classified as core or border points.\n",
    "   - Noise points are isolated from dense regions of the data and do not belong to any cluster.\n",
    "   - These noise points are often considered anomalies because they exhibit behavior that deviates from the majority of the data.\n",
    "\n",
    "2. **Key Parameters**:\n",
    "   - **Epsilon (ε)**: Specifies the maximum distance between two data points to consider them as neighbors. It defines the neighborhood size for density estimation. Smaller values of epsilon result in tighter clusters and may lead to more noise points being detected as anomalies.\n",
    "   - **MinPts**: Specifies the minimum number of data points within the epsilon distance to consider a point as a core point. It controls the density threshold for cluster formation. Larger values of MinPts result in more stringent density requirements and may reduce the likelihood of noise points being detected as anomalies.\n",
    "\n",
    "3. **Anomaly Detection**:\n",
    "   - Anomalies are indirectly detected by identifying noise points that are isolated from dense regions of the data.\n",
    "   - Data points that are not part of any cluster (i.e., noise points) are considered anomalies because they exhibit behavior that deviates from the majority of the data.\n",
    "   - The epsilon (ε) and MinPts parameters play a crucial role in determining the sensitivity of DBSCAN to noise points and, consequently, its ability to detect anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c0fd3f-df36-4f0a-81bd-194db1d8062a",
   "metadata": {},
   "source": [
    "# Q7. What is the make_circles package in scikit-learn used for?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4045c1a1-a620-4a96-91fb-25058d816f05",
   "metadata": {},
   "source": [
    "The `make_circles' function in scikit-learn is used to generate synthetic datasets consisting of concentric circles, which are often used for testing and illustrating clustering algorithms. Specifically, it generates a dataset with two features (X, Y coordinates) and assigns labels to data points indicating the concentric circle to which they belong. Here's a breakdown of its usage:\n",
    "\n",
    "1. **Generating Synthetic Data**:\n",
    "   - `make_circles` generates a synthetic dataset where data points are distributed in the shape of concentric circles.\n",
    "   - The dataset consists of two features (X, Y coordinates) representing the positions of the data points in the 2D space.\n",
    "   - Each data point is assigned a label indicating whether it belongs to the inner or outer circle.\n",
    "\n",
    "2. **Testing Clustering Algorithms**:\n",
    "   - The `make_circles` dataset is commonly used for testing and illustrating clustering algorithms, particularly those designed to identify non-linearly separable clusters.\n",
    "   - Algorithms like K-means may struggle to effectively cluster the data, while algorithms like DBSCAN or spectral clustering may perform better.\n",
    "\n",
    "3. **Visualizing Clustering Results**:\n",
    "   - Since the `make_circles` dataset is synthetic and its ground truth is known, it's useful for visualizing and evaluating the performance of clustering algorithms.\n",
    "   - Clustering results can be plotted alongside the ground truth to assess the algorithm's ability to correctly identify the underlying structure of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2ff518-d911-4a51-9616-8363e3710cfc",
   "metadata": {},
   "source": [
    "# Q8. What are local outliers and global outliers, and how do they differ from each other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9adaab4-0dae-4186-80d3-f59f744d2ccf",
   "metadata": {},
   "source": [
    "Local outliers and global outliers are two types of anomalies in a dataset, but they differ in terms of their context and characteristics:\n",
    "\n",
    "1. **Local Outliers**:\n",
    "   - Local outliers are data points that are significantly different from their local neighborhood but may not be anomalous in the global context of the dataset.\n",
    "   - These outliers are detected based on their deviation from the local density or behavior of neighboring data points.\n",
    "   - Local outliers are often identified using density-based anomaly detection algorithms like Local Outlier Factor (LOF) or k-nearest neighbors (KNN) approaches.\n",
    "   - Examples of local outliers include sudden spikes or dips in a time series, anomalies within clusters of similar data points, or isolated anomalies within dense regions of the dataset.\n",
    "\n",
    "2. **Global Outliers**:\n",
    "   - Global outliers, also known as global anomalies or global discordant points, are data points that are significantly different from the majority of the data points in the entire dataset.\n",
    "   - These outliers exhibit behavior that is anomalous when compared to the overall distribution of the data.\n",
    "   - Global outliers are typically detected based on their deviation from the overall statistical properties of the dataset, such as mean, median, variance, or distribution.\n",
    "   - Examples of global outliers include extreme values, outliers that are inconsistent with the general trend of the data, or anomalies that affect the entire dataset.\n",
    "\n",
    "**Key Differences**:\n",
    "   - **Context**: Local outliers are anomalous within a specific local neighborhood, while global outliers are anomalous in the overall context of the dataset.\n",
    "   - **Detection Method**: Local outliers are identified based on their deviation from the local density or behavior of neighboring data points, whereas global outliers are detected based on their deviation from the overall statistical properties of the dataset.\n",
    "   - **Impact**: Local outliers may not significantly affect the entire dataset but can be important in specific contexts or subgroups. In contrast, global outliers have a broader impact on the entire dataset and may indicate systemic issues or errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8719374-af95-45c3-ba40-b2b04b95cb3c",
   "metadata": {},
   "source": [
    "# Q9. How can local outliers be detected using the Local Outlier Factor (LOF) algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3503eed-bb29-483d-b298-86722f7bf521",
   "metadata": {},
   "source": [
    "The Local Outlier Factor (LOF) algorithm is a popular method for detecting local outliers in a dataset. Here's how LOF detects local outliers:\n",
    "\n",
    "1. **Local Density Estimation**:\n",
    "   - LOF computes the local density of each data point by comparing its distance to its \\( k \\) nearest neighbors. The local density of a point is the inverse of the average distance to its neighbors. Data points in dense regions have higher local densities, while points in sparse regions have lower local densities.\n",
    "\n",
    "2. **Relative Density**:\n",
    "   - For each data point \\( p \\), LOF compares its local density to the local densities of its neighbors. The relative density of \\( p \\) with respect to its neighbor \\( q \\) is the ratio of the local density of \\( p \\) to the local density of \\( q \\). This measures how much more dense \\( p \\) is compared to its neighbors.\n",
    "\n",
    "3. **Local Outlier Factor (LOF)**:\n",
    "   - The LOF of a data point \\( p \\) quantifies its degree of outlierliness based on its relative density compared to its neighbors. It is the average ratio of the relative densities of \\( p \\) with respect to its neighbors.\n",
    "   - Data points with significantly higher LOF scores than their neighbors are considered local outliers, as they have lower relative densities compared to their surroundings, indicating that they are less well-connected to their local neighborhoods.\n",
    "\n",
    "4. **Thresholding**:\n",
    "   - A threshold is typically applied to the LOF scores to identify local outliers. Data points with LOF scores exceeding the threshold are labeled as local outliers.\n",
    "\n",
    "5. **Parameter Selection**:\n",
    "   - The key parameter in the LOF algorithm is \\( k \\), which specifies the number of nearest neighbors used for density estimation. Selecting an appropriate value of \\( k \\) is crucial for the effectiveness of LOF in detecting local outliers. Larger values of \\( k \\) capture more local structure but may overlook small, isolated anomalies, while smaller values of \\( k \\) are more sensitive to local anomalies but may also increase false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd1fec2-43c2-4c98-8287-141518c6f819",
   "metadata": {},
   "source": [
    "# Q10. How can global outliers be detected using the Isolation Forest algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f5053a-df49-46f1-9f2e-a69c25e6bcf3",
   "metadata": {},
   "source": [
    "The Isolation Forest algorithm is primarily designed for detecting global outliers in a dataset. Here's how the Isolation Forest algorithm detects global outliers:\n",
    "\n",
    "1. **Random Partitioning**:\n",
    "   - The Isolation Forest algorithm randomly selects a feature and then randomly selects a split value between the minimum and maximum values of that feature to partition the data recursively.\n",
    "\n",
    "2. **Recursive Partitioning**:\n",
    "   - It recursively partitions the data into subspaces (or \"isolation trees\") by randomly selecting features and split values until each data point is isolated in its own partition.\n",
    "\n",
    "3. **Outlier Score Calculation**:\n",
    "   - The outlier score for each data point is calculated based on the average path length in the isolation trees. Data points that have shorter average path lengths are considered more likely to be outliers.\n",
    "   - In general, outliers are isolated more quickly than normal data points during the partitioning process. Therefore, the average path length to reach an outlier is expected to be shorter than that of a normal data point.\n",
    "\n",
    "4. **Thresholding**:\n",
    "   - A threshold is typically applied to the outlier scores to identify global outliers. Data points with outlier scores exceeding the threshold are labeled as global outliers.\n",
    "\n",
    "5. **Parameter Selection**:\n",
    "   - The key parameters in the Isolation Forest algorithm are the number of trees (n_estimators) and the subsample size (max_samples).\n",
    "   - Increasing the number of trees improves the accuracy of outlier detection but also increases computational overhead.\n",
    "   - The subsample size controls the number of samples used to build each isolation tree. Smaller subsample sizes can speed up training but may lead to less accurate outlier detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1561c0e2-e98b-44d7-a6b6-fed2b6ab7c67",
   "metadata": {},
   "source": [
    "# Q11. What are some real-world applications where local outlier detection is more appropriate than global outlier detection, and vice versa?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f097cf-a346-46d4-a9b7-383e98eda6f2",
   "metadata": {},
   "source": [
    "Local and global outlier detection techniques are suitable for different real-world applications depending on the nature of the data and the specific objectives of anomaly detection. Here are some examples of real-world applications where each type of outlier detection may be more appropriate:\n",
    "\n",
    "**Local Outlier Detection:**\n",
    "\n",
    "1. **Network Intrusion Detection**:\n",
    "   - In cybersecurity, detecting anomalous behavior in network traffic is critical for identifying potential security threats.\n",
    "   - Local outlier detection methods like Local Outlier Factor (LOF) can identify unusual patterns in network traffic that deviate from the norm within specific network segments or protocols.\n",
    "\n",
    "2. **Fraud Detection**:\n",
    "   - In financial transactions, fraudsters often attempt to hide their activities by blending in with normal behavior.\n",
    "   - Local outlier detection techniques are effective for identifying unusual transaction patterns that deviate from the typical behavior of individual account holders or small groups of accounts.\n",
    "\n",
    "3. **Healthcare Monitoring**:\n",
    "   - In healthcare, monitoring patient vital signs or physiological parameters in real-time is essential for detecting early signs of health deterioration.\n",
    "   - Local outlier detection methods can identify sudden and unexpected changes in patient data within short time intervals, such as anomalies in heart rate, blood pressure, or respiratory rate.\n",
    "\n",
    "**Global Outlier Detection:**\n",
    "\n",
    "1. **Manufacturing Quality Control**:\n",
    "   - In manufacturing processes, identifying defective products or equipment failures is crucial for maintaining product quality and minimizing downtime.\n",
    "   - Global outlier detection techniques are suitable for detecting anomalies that affect the overall production process, such as extreme deviations in product specifications or equipment performance metrics.\n",
    "\n",
    "2. **Environmental Monitoring**:\n",
    "   - In environmental monitoring, detecting unusual phenomena or pollution events in large geographical areas is important for ensuring public health and safety.\n",
    "   - Global outlier detection methods can identify anomalies that affect broad spatial or temporal scales, such as spikes in air pollution levels or abnormal weather patterns.\n",
    "\n",
    "3. **Credit Card Fraud Detection**:\n",
    "   - In credit card transactions, detecting fraudulent activities that span multiple accounts or geographical locations is essential for preventing financial losses.\n",
    "   - Global outlier detection techniques can identify coordinated fraud schemes or large-scale anomalies that involve multiple transactions or account holders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142aaf46-2d3b-414b-a089-72fa2088c758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

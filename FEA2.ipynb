{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "229152da-8661-434e-9177-0035d79f2869",
   "metadata": {},
   "source": [
    "## Q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5543c59e-109e-484b-8d0b-e1341d776ef9",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to scale numeric features in a specific range, usually between 0 and 1. The purpose of Min-Max scaling is to ensure that all the features have the same scale, preventing certain features from dominating others simply because of their larger magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbae9761-c110-49d4-84b2-97a02cf79398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf900bf9-0aef-4a1e-9ab0-0652c562735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([2,5,10,15,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c74b8d4-d894-47dd-af1d-134fcbdf8c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  5, 10, 15, 20])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6891d05e-7d0d-4ee0-bf33-2d48da2501f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value = np.min(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e3df896-7d62-49ad-bdf0-54b011e3e1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = np.max(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4c85efe-4129-45fd-b9c5-4c212414a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = (data - min_value)/(max_value - min_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97d95435-402e-40bd-8cb3-f428e243b699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data: [ 2  5 10 15 20]\n"
     ]
    }
   ],
   "source": [
    "print(\"original data:\",data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e5ca93b-e06c-476d-b72f-6113e2e096e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min-max scaled data: [0.         0.16666667 0.44444444 0.72222222 1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"min-max scaled data:\", scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cc560e-2cdd-46dc-b590-76e70dc6a4ac",
   "metadata": {},
   "source": [
    "## Q(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdff76ae-cb22-410f-a854-b60f7abd7517",
   "metadata": {},
   "source": [
    "The Unit Vector technique in feature scaling is also known as vector normalization or L2 normalization. It involves scaling individual samples to have a norm (length or magnitude) of 1. The purpose is to ensure that all samples have the same scale while preserving the direction of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938fffb4-e725-4102-9170-1f26b25b2e47",
   "metadata": {},
   "source": [
    "The key difference between Min-Max scaling and Unit Vector scaling lies in the transformation applied. Min-Max scaling scales the data to a specific range (e.g., between 0 and 1), while Unit Vector scaling normalizes the data such that each sample becomes a vector with a length of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab14d4bf-ea10-45ae-a2aa-f0fb08422602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af86c666-6830-4740-aa5f-e507d2af6334",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([2,5,10,15,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31c87ad2-c9f8-4fc2-aaad-fda072fc48a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_norm = np.linalg.norm(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "942b896f-6d68-4f06-88ef-13ec6ceea53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = data/vector_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04d2235b-9590-44e8-b187-a129bc0dff42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: [ 2  5 10 15 20]\n",
      "Unit Vector scaled data: [0.0728357  0.18208926 0.36417852 0.54626778 0.72835704]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original data:\", data)\n",
    "print(\"Unit Vector scaled data:\", normalized_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfd254c-ab76-4380-a6e3-34426616099a",
   "metadata": {},
   "source": [
    "## Q(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04bcdec-b72b-453a-9a7e-a2bd8fa2f2ec",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a dimensionality reduction technique commonly used in the field of machine learning and statistics. Its primary goal is to transform high-dimensional data into a lower-dimensional representation, capturing the most significant variance in the data. This is achieved by identifying the principal components, which are linear combinations of the original features.\n",
    "\n",
    "Here's a step-by-step explanation of how PCA works and an example to illustrate its application:\n",
    "\n",
    "### How PCA Works:\n",
    "1. **Standardization:**\n",
    "   - If the features have different scales, it's essential to standardize them (subtract the mean and divide by the standard deviation) to ensure that each feature contributes equally to the analysis.\n",
    "\n",
    "2. **Covariance Matrix:**\n",
    "   - Compute the covariance matrix of the standardized data. The covariance matrix provides information about the relationships between different features.\n",
    "\n",
    "3. **Eigenvalue and Eigenvector Decomposition:**\n",
    "   - Find the eigenvalues and eigenvectors of the covariance matrix. The eigenvectors represent the principal components, and the corresponding eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "4. **Sort Eigenvectors by Eigenvalues:**\n",
    "   - Arrange the eigenvectors in descending order based on their corresponding eigenvalues. The eigenvector with the highest eigenvalue is the first principal component, the second-highest is the second principal component, and so on.\n",
    "\n",
    "5. **Select Principal Components:**\n",
    "   - Choose the top k eigenvectors to form a new matrix (projection matrix) where k is the desired dimensionality of the reduced data.\n",
    "\n",
    "6. **Transform the Data:**\n",
    "   - Multiply the original standardized data by the projection matrix to obtain the lower-dimensional representation of the data.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Let's consider a dataset with two features: height (in inches) and weight (in pounds) of a group of individuals. We want to reduce the dimensionality to one dimension using PCA.\n",
    "\n",
    "1. **Standardize the Data:**\n",
    "   - Subtract the mean and divide by the standard deviation for both height and weight.\n",
    "\n",
    "2. **Covariance Matrix:**\n",
    "   - Compute the covariance matrix based on the standardized data.\n",
    "\n",
    "3. **Eigenvalue and Eigenvector Decomposition:**\n",
    "   - Find the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "4. **Sort Eigenvectors by Eigenvalues:**\n",
    "   - Sort the eigenvectors in descending order.\n",
    "\n",
    "5. **Select Principal Components:**\n",
    "   - Choose the top eigenvector as the principal component.\n",
    "\n",
    "6. **Transform the Data:**\n",
    "   - Multiply the original standardized data by the selected eigenvector to obtain the one-dimensional representation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d602fe-ce33-4b36-9c00-54c5cd8d1979",
   "metadata": {},
   "source": [
    "## Q(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec13ad92-f43a-4f7f-8b60-2fec918f841d",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is closely related to feature extraction, and in many cases, PCA is used explicitly for feature extraction. Feature extraction involves transforming the original features of a dataset into a new set of features, usually with the goal of reducing dimensionality, capturing relevant information, or enhancing the performance of machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bc1ca4-72f0-4cdb-8ffd-30c239efdd49",
   "metadata": {},
   "source": [
    "Relationship Between PCA and Feature Extraction:\n",
    "Dimensionality Reduction:\n",
    "\n",
    "PCA aims to reduce the dimensionality of the data by transforming it into a new set of features (principal components) that capture the most significant variance in the original data.\n",
    "Decorrelation of Features:\n",
    "\n",
    "PCA also has the property of decorrelating the features, meaning that the principal components are orthogonal (uncorrelated). This can be advantageous when dealing with multicollinearity in the original feature space.\n",
    "Variance Retention:\n",
    "\n",
    "The principal components are ordered by the amount of variance they explain. By selecting a subset of the top principal components, you can retain most of the important information in the data while discarding less important, noisy, or redundant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9d4120-304e-4910-9c24-dcbbbaa2d67f",
   "metadata": {},
   "source": [
    "Standardize the Data:\n",
    "\n",
    "Standardize the length, width, and height to ensure they have the same scale.\n",
    "Apply PCA:\n",
    "\n",
    "Use PCA to find the principal components of the standardized data. Let's say the first principal component captures 80% of the variance, and the second principal component captures 15% of the variance.\n",
    "Select Principal Components:\n",
    "\n",
    "Choose the top two principal components as the new features for the dataset. These components are linear combinations of the original length, width, and height.\n",
    "Transform the Data:\n",
    "\n",
    "Multiply the standardized data by the selected principal components to obtain the two-dimensional representation of the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e66ba6-e9c4-44f6-abb6-de776f974b72",
   "metadata": {},
   "source": [
    "## Q(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c869c762-6449-44fd-99b5-bcf7e7783a82",
   "metadata": {},
   "source": [
    "Min-Max scaling is a data preprocessing technique used to scale numerical features to a specific range, typically between 0 and 1. This normalization method is particularly useful when the features in a dataset have different scales, and it helps ensure that all features contribute equally to the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04ebb14d-96a7-4127-823c-57ea6eca38e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2a747f9-ebd2-4a3e-a7c6-86f2697ed3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'price': [10.0, 25.0, 15.0, 30.0],\n",
    "    'rating': [4.2, 3.5, 4.8, 4.0],\n",
    "    'delivery_time': [20, 40, 30, 25]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1b21683-0645-4cec-b1ce-816b06b9b5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a53159e-c219-4049-85e7-ea7f47085168",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_scale = ['price', 'rating', 'delivery_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a54b43a-2480-4e5e-9001-23f5b10ee936",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b180cda-7658-46c7-82fb-767bedd21cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[features_to_scale] = scaler.fit_transform(df[features_to_scale])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0a5dc61-f754-40e6-8977-e389cb68d93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Data:\n",
      "   price    rating  delivery_time\n",
      "0   0.00  0.538462           0.00\n",
      "1   0.75  0.000000           1.00\n",
      "2   0.25  1.000000           0.50\n",
      "3   1.00  0.384615           0.25\n"
     ]
    }
   ],
   "source": [
    "print(\"Scaled Data:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6992dc-ee6d-492b-b1ee-0d12da10220d",
   "metadata": {},
   "source": [
    "## Q(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0336d836-b7b3-423d-a1b2-59006b7b9a7f",
   "metadata": {},
   "source": [
    "When dealing with a dataset with many features, such as company financial data and market trends for predicting stock prices, Principal Component Analysis (PCA) can be a valuable tool for reducing dimensionality. The primary objective is to transform the original features into a smaller set of uncorrelated variables (principal components) that retain most of the essential information in the data. Here's how you could use PCA for dimensionality reduction in the context of predicting stock prices:\n",
    "\n",
    "Steps to Use PCA for Dimensionality Reduction:\n",
    "\n",
    "Data Preprocessing:\n",
    "\n",
    "Ensure that your dataset is prepared for analysis. Handle missing values, normalize or standardize the data if necessary, and address any other preprocessing steps.\n",
    "\n",
    "Feature Standardization:\n",
    "\n",
    "Standardize the features to ensure that they all have the same scale. This step is crucial for PCA since it is sensitive to the scale of the features. Standardization involves subtracting the mean and dividing by the standard deviation for each feature.\n",
    "\n",
    "Apply PCA:\n",
    "\n",
    "Use PCA to identify the principal components of the standardized data. The principal components are linear combinations of the original features, sorted by the amount of variance they explain.\n",
    "\n",
    "Determine the Number of Components:\n",
    "\n",
    "Examine the explained variance ratio to decide on the number of principal components to retain. The explained variance ratio indicates the proportion of the total variance in the data that is explained by each principal component. You can set a threshold (e.g., 95% of variance explained) to determine the number of components to keep.\n",
    "Project Data onto Principal Components:\n",
    "\n",
    "Transform the original dataset by projecting it onto the selected principal components. This results in a reduced-dimensional representation of the data.\n",
    "\n",
    "Model Training:\n",
    "\n",
    "Train your stock price prediction model on the dataset with reduced dimensionality. You can use various machine learning algorithms for regression or time series forecasting, depending on the nature of your prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3aa5a4-cb68-4039-a73a-26c5022ac9e8",
   "metadata": {},
   "source": [
    "## Q(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a7f20f7-e744-48e4-8317-4f23bf0ec862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deb8d176-9754-4126-8379-464816162bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = np.array([1, 5, 10, 15, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b29ab588-f5cc-4253-a42c-1bde36085831",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_min = -1\n",
    "new_max = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c86e93c7-0ff1-4a98-99bd-9640ab25bc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val = np.min(original_data)\n",
    "max_val = np.max(original_data)\n",
    "scaled_data = ((original_data - min_val) / (max_val - min_val)) * (new_max - new_min) + new_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d252740e-8214-4609-bce6-35bf2c3e153c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data: [ 1  5 10 15 20]\n",
      "Min-Max Scaled Data: [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Data:\", original_data)\n",
    "print(\"Min-Max Scaled Data:\", scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d474393-706c-4092-92b3-8d89cda1e27a",
   "metadata": {},
   "source": [
    "## Q(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506e126b-37ef-468c-81d3-027edd594adb",
   "metadata": {},
   "source": [
    "\n",
    "The decision on how many principal components to retain in a PCA-based feature extraction process depends on the amount of variance you want to preserve in the dataset. Typically, you aim to retain a sufficiently high percentage of the total variance to capture the essential information in the data. A common approach is to set a threshold, such as retaining 95% or 99% of the total variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5bbc48-4743-4fff-b5f0-243e88026038",
   "metadata": {},
   "source": [
    "Here's a general guide on how to determine the number of principal components to retain:\n",
    "\n",
    "Compute the Explained Variance Ratio:\n",
    "\n",
    "After applying PCA to your dataset, the explained_variance_ratio_ attribute of the PCA object will provide the proportion of the dataset's variance explained by each principal component.\n",
    "\n",
    "Cumulative Explained Variance:\n",
    "\n",
    "Calculate the cumulative explained variance by summing the explained variance ratios as you go through the principal components.\n",
    "\n",
    "Choose the Number of Components:\n",
    "\n",
    "Decide on the number of principal components to retain based on the cumulative explained variance. A common threshold is to retain enough components to reach a cumulative explained variance of 95% or 99%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f66031a-f8dc-4dbe-8929-895c9504f0ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

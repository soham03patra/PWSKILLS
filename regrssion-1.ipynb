{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5a3f372-5860-457e-9824-9a57bbcb81d7",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4c4ab6-0a74-4801-8fa2-09866a84cae8",
   "metadata": {},
   "source": [
    "Simple Linear Regression:\n",
    "Definition: Simple linear regression models the relationship between one dependent variable (Y) and one independent variable (X).\n",
    "Equation: The mathematical representation of simple linear regression is: [ Y = C0 + C1X]\n",
    "\n",
    "(Y): Dependent variable (target variable)\n",
    "(X): Independent variable (input variable)\n",
    "(C_0): Intercept (value of (Y) when (X=0))\n",
    "\n",
    "Use Case: Simple linear regression is suitable when there is one clear predictor influencing the outcome.\n",
    "Visualization: Typically visualized with a 2D scatter plot and a line of best fit.\n",
    "Risk of Overfitting: Lower, as it deals with only one predictor.\n",
    "Assumptions: Linearity, independence, homoscedasticity, and normality.\n",
    "\n",
    "Multiple Linear Regression:\n",
    "Definition: Multiple linear regression models the relationship between one dependent variable (Y) and two or more independent variables (X1, X2, X3, ‚Ä¶).\n",
    "\n",
    "Equation: \n",
    "The mathematical representation of multiple linear regression is: [ Y = C_0 + C_1X_1 + C_2X_2 + C_3X_3 + \\ldots + C_nX_n]\n",
    "\n",
    "(X_1, X_2, X_3, \\ldots, X_n): Multiple independent variables\n",
    "(C_0, C_1, C_2, C_3, \\ldots, C_n): Coefficients\n",
    "\n",
    "Use Case: Multiple linear regression is suitable when multiple factors affect the outcome.\n",
    "Visualization: Requires 3D or multi-dimensional space, often represented using partial regression plots.\n",
    "Risk of Overfitting: Higher, especially if too many predictors are used without adequate data.\n",
    "Assumptions: Same as linear regression, with the added concern of multicollinearity.\n",
    "\n",
    "Example:\n",
    "Let‚Äôs consider an example:\n",
    "\n",
    "Simple Linear Regression: Suppose we want to predict a student‚Äôs final exam score ((Y)) based on the number of hours they studied ((X)). Here, we have only one predictor (study hours).\n",
    "Multiple Linear Regression: Imagine predicting a house‚Äôs sale price ((Y)) based on features like square footage ((X_1)), number of bedrooms ((X_2)), and neighborhood safety rating ((X_3)). In this case, we have multiple predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df522c00-4925-4ef9-8b34-7628e329432f",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05922903-920f-4247-ba8c-5f579211b24f",
   "metadata": {},
   "source": [
    "Linear regression makes several assumptions about the data:\n",
    "\n",
    "Linearity: The relationship between the independent variables (features) and the dependent variable (target) should be linear. This means that the change in the dependent variable is proportional to the change in the independent variable.\n",
    "\n",
    "Independence: The observations should be independent of each other. This means that the value of one observation should not be influenced by the value of another observation.\n",
    "\n",
    "Homoscedasticity (Constant Variance): The variance of the residuals (the differences between the observed and predicted values) should be constant across all levels of the independent variables. In other words, the spread of the residuals should remain constant as the value of the independent variable changes.\n",
    "\n",
    "Normality of Residuals: The residuals should be normally distributed. This means that the distribution of the residuals should resemble a bell-shaped curve when plotted.\n",
    "\n",
    "No Multicollinearity: There should be no multicollinearity among the independent variables. Multicollinearity occurs when two or more independent variables are highly correlated with each other.\n",
    "\n",
    "To check whether these assumptions hold in a given dataset, you can perform various diagnostic tests:\n",
    "\n",
    "Residual Analysis: Plot the residuals against the predicted values and independent variables. The plots should exhibit no discernible pattern, indicating that the assumptions of linearity and homoscedasticity are met.\n",
    "\n",
    "Normality Tests: Perform statistical tests, such as the Shapiro-Wilk test or Kolmogorov-Smirnov test, to check the normality of residuals. Additionally, you can visually inspect a histogram or a Q-Q plot of the residuals to assess their distribution.\n",
    "\n",
    "VIF (Variance Inflation Factor): Calculate the VIF for each independent variable to detect multicollinearity. VIF values greater than 10 indicate problematic levels of multicollinearity.\n",
    "\n",
    "Durbin-Watson Test: This test helps to detect autocorrelation in the residuals. If the Durbin-Watson statistic deviates significantly from 2, it suggests the presence of autocorrelation.\n",
    "\n",
    "Cook‚Äôs Distance: This diagnostic measure helps identify influential data points that may disproportionately affect the regression coefficients. Points with high Cook's distance may warrant further investigation.\n",
    "\n",
    "Heteroscedasticity Tests: Conduct statistical tests like the Breusch-Pagan test or White test to formally assess homoscedasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a78489-065a-4cbb-b3ab-3b487d87d347",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b48d097-e2e4-4f99-9373-4f38a537087e",
   "metadata": {},
   "source": [
    "In a linear regression model of the form \n",
    "ùëå\n",
    "=\n",
    "ùõΩ\n",
    "0\n",
    "+\n",
    "ùõΩ\n",
    "1\n",
    "ùëã\n",
    "+\n",
    "ùúñ\n",
    "Y=Œ≤ \n",
    "0\n",
    "‚Äã\n",
    " +Œ≤ \n",
    "1\n",
    "‚Äã\n",
    " X+œµ, the slope (Œ≤‚ÇÅ) represents the change in the dependent variable (Y) for a one-unit change in the independent variable (X), holding all other variables constant. The intercept (Œ≤‚ÇÄ) represents the value of the dependent variable (Y) when the independent variable (X) is zero.\n",
    "\n",
    "Here's an example to illustrate the interpretation of slope and intercept in a real-world scenario:\n",
    "\n",
    "Scenario: Suppose you want to predict the price of a house based on its size (in square feet). You collect data on house prices and sizes and fit a linear regression model.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "Intercept (\n",
    "ùõΩ\n",
    "0\n",
    "Œ≤ \n",
    "0\n",
    "‚Äã\n",
    " ): Let's say the intercept of your regression model is $50,000. This means that when the size of the house (X) is zero, the predicted price of the house (Y) is $50,000. However, this interpretation might not make sense in the context of house prices, as a house with zero square feet is unlikely.\n",
    "Slope (\n",
    "ùõΩ\n",
    "1\n",
    "Œ≤ \n",
    "1\n",
    "‚Äã\n",
    " ): Suppose the slope of your regression model is 100. This means that for every one-unit increase in the size of the house (e.g., one additional square foot), the predicted price of the house increases by $100, holding all other factors constant. So, if a house is 100 square feet larger than another house, you would expect it to be priced $10,000 higher.\n",
    "Example Calculation:\n",
    "\n",
    "If a house has a size of 1500 square feet, using the intercept and slope mentioned above:\n",
    "Price\n",
    "=\n",
    "$\n",
    "50\n",
    ",\n",
    "000\n",
    "+\n",
    "100\n",
    "√ó\n",
    "1500\n",
    "Price=$50,000+100√ó1500\n",
    "Price\n",
    "=\n",
    "$\n",
    "50\n",
    ",\n",
    "000\n",
    "+\n",
    "150\n",
    ",\n",
    "000\n",
    "Price=$50,000+150,000\n",
    "Price\n",
    "=\n",
    "$\n",
    "200\n",
    ",\n",
    "000\n",
    "Price=$200,000\n",
    "So, according to this model, a house with a size of 1500 square feet would be predicted to have a price of $200,000.\n",
    "\n",
    "Interpreting the slope and intercept in a linear regression model allows us to understand how changes in the independent variable (X) relate to changes in the dependent variable (Y) and provides insights into the relationship between the variables in the context of the real-world scenario under consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b56160e-d16b-4642-a9ca-ae75e276b5ca",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40e35f9-966a-487b-9ee8-2cd356a33b9e",
   "metadata": {},
   "source": [
    "Gradient descent is an optimization algorithm used to minimize the cost function of a model by iteratively adjusting its parameters. It's a fundamental technique in machine learning, especially in training models such as linear regression, logistic regression, neural networks, and many others.\n",
    "\n",
    "Here's how gradient descent works:\n",
    "\n",
    "Initialization: Gradient descent starts by initializing the parameters of the model with some arbitrary values. These parameters are the weights and biases associated with the features in the model.\n",
    "\n",
    "Calculate the Gradient: The algorithm then calculates the gradient of the cost function with respect to each parameter. The gradient points in the direction of the steepest increase of the cost function.\n",
    "\n",
    "Update Parameters: Once the gradients are calculated, the algorithm updates the parameters in the opposite direction of the gradient to minimize the cost function. This is done iteratively by taking steps proportional to the negative of the gradient.\n",
    "\n",
    "Convergence: The process continues iteratively until the algorithm converges to a minimum of the cost function, or until a predefined number of iterations is reached.\n",
    "\n",
    "There are different variants of gradient descent, including:\n",
    "\n",
    "Batch Gradient Descent: In this variant, the gradient is computed using the entire dataset. It provides a more accurate estimate of the gradient but can be computationally expensive for large datasets.\n",
    "\n",
    "Stochastic Gradient Descent (SGD): SGD computes the gradient using only one training example at a time. It's faster than batch gradient descent but can be noisy.\n",
    "\n",
    "Mini-batch Gradient Descent: Mini-batch gradient descent is a compromise between batch and stochastic gradient descent. It computes the gradient using a small random subset of the training data. It combines the advantages of both batch and stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9858dc7-bbb4-41cc-ba7d-ee99775ec05e",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abb4211-afac-4b20-bbfb-cec17329a125",
   "metadata": {},
   "source": [
    "Multiple linear regression is an extension of simple linear regression that allows for the prediction of a dependent variable based on two or more independent variables. While simple linear regression involves only one independent variable, multiple linear regression incorporates multiple predictors.\n",
    "\n",
    "The multiple linear regression model can be represented as:\n",
    "\n",
    " y=\n",
    "ùõΩ\n",
    "0\n",
    "+\n",
    "ùõΩ\n",
    "1\n",
    "ùëã\n",
    "1\n",
    "+\n",
    "ùõΩ\n",
    "2\n",
    "ùëã\n",
    "2\n",
    "+\n",
    "‚Ä¶\n",
    "+\n",
    "ùõΩ\n",
    "ùëù\n",
    "ùëã\n",
    "ùëù\n",
    "\n",
    "Y=Œ≤ \n",
    "0\n",
    "‚Äã\n",
    " +Œ≤ \n",
    "1\n",
    "‚Äã\n",
    " X \n",
    "1\n",
    "‚Äã\n",
    " +Œ≤ \n",
    "2\n",
    "‚Äã\n",
    " X \n",
    "2\n",
    "‚Äã\n",
    " +‚Ä¶+Œ≤ \n",
    "p\n",
    "‚Äã\n",
    " X \n",
    "p\n",
    "‚Äã\n",
    "\n",
    "Where:\n",
    "\n",
    "ùëå\n",
    "Y is the dependent variable.\n",
    "ùëã\n",
    "1\n",
    ",\n",
    "ùëã\n",
    "2\n",
    ",\n",
    "‚Ä¶\n",
    ",\n",
    "ùëã\n",
    "ùëù\n",
    "X \n",
    "1\n",
    "‚Äã\n",
    " ,X \n",
    "2\n",
    "‚Äã\n",
    " ,‚Ä¶,X \n",
    "p\n",
    "‚Äã\n",
    "  are the independent variables.\n",
    "ùõΩ\n",
    "0\n",
    "Œ≤ \n",
    "0\n",
    "‚Äã\n",
    "  is the intercept.\n",
    "ùõΩ\n",
    "1\n",
    ",\n",
    "ùõΩ\n",
    "2\n",
    ",\n",
    "‚Ä¶\n",
    ",\n",
    "ùõΩ\n",
    "ùëù\n",
    "Œ≤ \n",
    "1\n",
    "‚Äã\n",
    " ,Œ≤ \n",
    "2\n",
    "‚Äã\n",
    " ,‚Ä¶,Œ≤ \n",
    "p\n",
    "‚Äã\n",
    "  are the coefficients associated with each independent variable.\n",
    "\n",
    "Here's how multiple linear regression differs from simple linear regression:\n",
    "\n",
    "Number of Predictors: In simple linear regression, there is only one independent variable. However, in multiple linear regression, there are two or more independent variables. This allows for a more complex modeling of the relationship between the dependent and independent variables.\n",
    "Model Complexity: With multiple linear regression, the model can capture more complex relationships between the dependent variable and the predictors. It can account for interactions and nonlinear effects between the independent variables, providing a more flexible framework for modeling real-world phenomena.\n",
    "Interpretation of Coefficients: In simple linear regression, the coefficient represents the change in the dependent variable for a one-unit change in the independent variable. In multiple linear regression, the interpretation becomes more nuanced, as the coefficient represents the change in the dependent variable for a one-unit change in the corresponding independent variable, holding all other variables constant.\n",
    "Assumptions and Diagnostics: The assumptions of multiple linear regression are similar to those of simple linear regression, but the diagnostic procedures become more complex due to the presence of multiple predictors. Multicollinearity, for example, becomes a concern when predictors are correlated with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df8d5b2-bc7a-46fb-8bd6-d681b904d3ff",
   "metadata": {},
   "source": [
    "# Q6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a681be7d-3d5f-468c-a6d3-25f3c66d4b31",
   "metadata": {},
   "source": [
    "Multicollinearity in multiple linear regression occurs when two or more independent variables in the model are highly correlated with each other. This can cause issues in the estimation of the regression coefficients, leading to unstable parameter estimates and inflated standard errors. Multicollinearity can make it difficult to interpret the individual effects of the independent variables on the dependent variable.\n",
    "\n",
    "Here's how multicollinearity manifests and how it can affect the regression model:\n",
    "\n",
    "High Correlation Among Predictors: Multicollinearity is indicated by a high correlation between two or more independent variables. When predictors are highly correlated, it becomes challenging for the model to disentangle their individual effects on the dependent variable.\n",
    "\n",
    "Inflated Standard Errors: Multicollinearity leads to inflated standard errors of the regression coefficients. This means that the estimates of the coefficients become less precise, making it harder to determine the statistical significance of the predictors.\n",
    "\n",
    "Unstable Coefficients: Small changes in the data or the model specification can lead to large changes in the estimated coefficients. This instability makes it difficult to rely on the coefficients for making predictions or interpreting the relationships between variables.\n",
    "\n",
    "To detect multicollinearity, you can use several diagnostic techniques:\n",
    "\n",
    "Correlation Matrix: Calculate the correlation matrix among the independent variables. High correlation coefficients (typically above 0.7 or 0.8) indicate potential multicollinearity.\n",
    "\n",
    "Variance Inflation Factor (VIF): Calculate the VIF for each independent variable. VIF measures how much the variance of the coefficient estimate is inflated due to multicollinearity. VIF values greater than 10 (some suggest 5) are often considered indicative of multicollinearity.\n",
    "\n",
    "Eigenvalues: Compute the eigenvalues of the correlation matrix. If there are one or more eigenvalues close to zero, it indicates the presence of multicollinearity.\n",
    "\n",
    "To address multicollinearity, you can consider the following strategies:\n",
    "\n",
    "Remove Highly Correlated Predictors: If two or more predictors are highly correlated, you can consider removing one of them from the model.\n",
    "Combine Variables: If possible, you can combine highly correlated predictors into a single composite variable.\n",
    "\n",
    "Regularization Techniques: Techniques like ridge regression and Lasso regression introduce a penalty term to the regression coefficients, which can mitigate the effects of multicollinearity.\n",
    "\n",
    "Principal Component Analysis (PCA): PCA can be used to reduce the dimensionality of the data by transforming the original predictors into a smaller set of orthogonal components, which can help alleviate multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137a4332-55af-4575-a918-4aff313f74e0",
   "metadata": {},
   "source": [
    "# Q7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ac104c-302a-4b8d-8fff-2f4381af59d2",
   "metadata": {},
   "source": [
    "Polynomial regression is a form of regression analysis in which the relationship between the independent variable (predictor) and the dependent variable (response) is modeled as an nth-degree polynomial. Unlike linear regression, which assumes a linear relationship between the variables, polynomial regression can capture more complex and nonlinear relationships.\n",
    "\n",
    "The polynomial regression model can be represented as:\n",
    "\n",
    "y = \n",
    "ùõΩ\n",
    "0\n",
    "+\n",
    "ùõΩ\n",
    "1\n",
    "ùëã\n",
    "+\n",
    "ùõΩ\n",
    "2\n",
    "ùëã\n",
    "2\n",
    "+\n",
    "‚Ä¶\n",
    "+\n",
    "ùõΩ\n",
    "ùëõ\n",
    "ùëã\n",
    "ùëõ\n",
    "+\n",
    "ùúñ\n",
    "Y=Œ≤ \n",
    "0\n",
    "‚Äã\n",
    " +Œ≤ \n",
    "1\n",
    "‚Äã\n",
    " X+Œ≤ \n",
    "2\n",
    "‚Äã\n",
    " X \n",
    "2\n",
    " +‚Ä¶+Œ≤ \n",
    "n\n",
    "‚Äã\n",
    " X \n",
    "n\n",
    " +œµ\n",
    "\n",
    "Where:\n",
    "\n",
    "ùëå\n",
    "Y is the dependent variable.\n",
    "ùëã\n",
    "X is the independent variable.\n",
    "ùõΩ\n",
    "0\n",
    ",\n",
    "ùõΩ\n",
    "1\n",
    ",\n",
    "‚Ä¶\n",
    ",\n",
    "ùõΩ\n",
    "ùëõ\n",
    "Œ≤ \n",
    "0\n",
    "‚Äã\n",
    " ,Œ≤ \n",
    "1\n",
    "‚Äã\n",
    " ,‚Ä¶,Œ≤ \n",
    "n\n",
    "‚Äã\n",
    "  are the coefficients of the polynomial terms.\n",
    "ùúñ\n",
    "œµ is the error term.\n",
    "Polynomial regression allows us to fit a curve to the data instead of a straight line, making it useful for modeling relationships that are not linear. For example, if the relationship between the dependent and independent variables appears to be curved or quadratic, polynomial regression can capture this curvature by including higher-order polynomial terms.\n",
    "\n",
    "Here's how polynomial regression differs from linear regression:\n",
    "\n",
    "Model Complexity: Polynomial regression allows for more flexible modeling of the relationship between the variables. By including higher-order polynomial terms (e.g., \n",
    "ùëã\n",
    "2\n",
    ",\n",
    "ùëã\n",
    "3\n",
    "X \n",
    "2\n",
    " ,X \n",
    "3\n",
    " ), the model can capture nonlinear patterns in the data that linear regression cannot.\n",
    "Curvature: While linear regression assumes a linear relationship between the variables, polynomial regression can model curved relationships. This makes polynomial regression suitable for datasets where the relationship between the variables exhibits curvature or nonlinearity.\n",
    "Interpretation: In linear regression, the coefficients represent the change in the dependent variable for a one-unit change in the independent variable. In polynomial regression, interpreting the coefficients becomes more complex as higher-order terms are included. The coefficients associated with the polynomial terms indicate how the rate of change of the dependent variable varies with changes in the independent variable.\n",
    "Overfitting: Polynomial regression can be prone to overfitting, especially when high-degree polynomials are used. Overfitting occurs when the model captures noise or random fluctuations in the data instead of the underlying relationship. Regularization techniques such as ridge regression or cross-validation can help mitigate overfitting in polynomial regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06930d4-bab0-446e-9ba8-7f8a99b94113",
   "metadata": {},
   "source": [
    "# Q8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc229063-dc58-41d9-b58b-6c8a840fadd9",
   "metadata": {},
   "source": [
    "\n",
    "Polynomial regression offers several advantages and disadvantages compared to linear regression:\n",
    "\n",
    "Advantages of Polynomial Regression:\n",
    "\n",
    "Flexibility: Polynomial regression can capture nonlinear relationships between variables, providing a more flexible modeling approach compared to linear regression. It can fit curves to the data instead of assuming a straight line, allowing for a better representation of complex patterns.\n",
    "\n",
    "Improved Fit: In situations where the relationship between the variables is nonlinear, polynomial regression can provide a better fit to the data than linear regression. By including higher-order polynomial terms, the model can closely approximate the underlying relationship between the variables.\n",
    "\n",
    "Variable Importance: Polynomial regression can help identify the importance of different polynomial terms in predicting the dependent variable. It allows for the examination of how the rate of change of the dependent variable varies with changes in the independent variable.\n",
    "\n",
    "Disadvantages of Polynomial Regression:\n",
    "\n",
    "Overfitting: Polynomial regression, especially with high-degree polynomials, is prone to overfitting. Overfitting occurs when the model captures noise or random fluctuations in the data instead of the underlying relationship. It can lead to poor generalization performance on unseen data.\n",
    "\n",
    "Interpretability: As the degree of the polynomial increases, interpreting the coefficients of the model becomes more challenging. Higher-order polynomial terms can lead to complex models that are difficult to interpret and explain.\n",
    "\n",
    "Extrapolation: Polynomial regression may not be suitable for extrapolation beyond the range of the observed data. Extrapolating with polynomial models can lead to unreliable predictions, especially if the underlying relationship is not well understood.\n",
    "\n",
    "Situation for Preferable Use of Polynomial Regression:\n",
    "\n",
    "Polynomial regression is preferred over linear regression in the following situations:\n",
    "\n",
    "Nonlinear Relationships: When the relationship between the dependent and independent variables is nonlinear, polynomial regression can provide a better fit to the data. It allows for the modeling of curved or quadratic relationships that linear regression cannot capture.\n",
    "\n",
    "Flexibility in Modeling: Polynomial regression is suitable when you want to model complex patterns in the data, such as curves or peaks. It offers flexibility in modeling by allowing the inclusion of higher-order polynomial terms to better represent the underlying relationship.\n",
    "\n",
    "Exploratory Data Analysis: In exploratory data analysis, polynomial regression can be useful for examining the shape of the relationship between variables and identifying potential nonlinearities. It can provide insights into the nature of the data before building more complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c720cc27-d378-41f9-b946-1b286df11a91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

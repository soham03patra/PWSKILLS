{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb518abf-3924-4a94-88a5-276ab4a8d153",
   "metadata": {},
   "source": [
    "# Q1. What is anomaly detection and what is its purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c857fa72-b953-485d-903d-b7ee6ffe8ea2",
   "metadata": {},
   "source": [
    "1. **Identification**: Anomaly detection is about spotting unusual patterns or outliers in data.\n",
    "  \n",
    "2. **Purpose**: It helps in various domains:\n",
    "    - **Fraud Detection**: Finding abnormal behavior in financial transactions.\n",
    "    - **Security**: Detecting cyber attacks or network intrusions.\n",
    "    - **Fault Detection**: Spotting malfunctions in industrial processes or equipment.\n",
    "    - **Healthcare**: Identifying anomalies in patient data.\n",
    "    - **Predictive Maintenance**: Anticipating equipment failures.\n",
    "    - **Quality Control**: Detecting defects in manufacturing.\n",
    "    - **Environmental Monitoring**: Noticing irregularities in environmental data.\n",
    "  \n",
    "3. **Benefits**: Improves efficiency, reduces risks, enhances security, and enables better decision-making by flagging unusual occurrences that require attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3f4d44-1c4b-412d-905a-a7e253ff42fe",
   "metadata": {},
   "source": [
    "# Q2. What are the key challenges in anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff40f0bc-f30f-4204-a4f0-c1aa42616c2c",
   "metadata": {},
   "source": [
    "1. **Labeling**: Lack of labeled data for training models, especially in unsupervised or semi-supervised anomaly detection scenarios.\n",
    "\n",
    "2. **Imbalanced Data**: Anomalies are often rare events, leading to imbalanced datasets where normal instances outnumber anomalies, making it difficult for models to learn effectively.\n",
    "\n",
    "3. **Noise and Variability**: Data may contain noise or variability that can make it challenging to distinguish between anomalies and normal behavior.\n",
    "\n",
    "4. **Contextual Understanding**: Understanding the context in which anomalies occur is crucial for accurate detection, but it can be complex and domain-specific.\n",
    "\n",
    "5. **Adaptability**: Anomaly detection models may struggle to adapt to changing environments or evolving patterns of normal and abnormal behavior.\n",
    "\n",
    "6. **Interpretability**: Some anomaly detection methods, especially deep learning approaches, lack interpretability, making it difficult to understand why certain instances are flagged as anomalies.\n",
    "\n",
    "7. **Scalability**: Efficiently handling large-scale datasets and real-time streaming data can be challenging for anomaly detection algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feb1d8e-fa9c-423d-90ed-0be76561be00",
   "metadata": {},
   "source": [
    "# Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdada29-25eb-4dba-9d2a-51a0cdfef3fb",
   "metadata": {},
   "source": [
    "1. **Unsupervised Anomaly Detection**:\n",
    "   - **No Labeled Data**: Unsupervised methods don't rely on labeled data during training. They learn patterns from the input data without explicitly being told what constitutes normal or anomalous behavior.\n",
    "   - **Detects Unknown Anomalies**: Since unsupervised methods don't require labeled anomalies, they can detect unknown or novel anomalies that were not seen during training.\n",
    "   - **Examples**: Clustering-based methods (e.g., k-means), density-based methods (e.g., DBSCAN), and reconstruction-based methods (e.g., autoencoders) are commonly used unsupervised anomaly detection techniques.\n",
    "\n",
    "2. **Supervised Anomaly Detection**:\n",
    "   - **Uses Labeled Data**: Supervised methods rely on labeled data during training, where both normal and anomalous instances are explicitly labeled.\n",
    "   - **Classifies Anomalies**: These methods learn to distinguish between normal and anomalous instances based on the labeled examples provided during training.\n",
    "   - **Limited to Known Anomalies**: Supervised methods are limited to detecting anomalies similar to those seen during training, as they require labeled examples of anomalies to learn from.\n",
    "   - **Examples**: Classification algorithms (e.g., Support Vector Machines, Random Forests, Neural Networks) trained on labeled data are commonly used in supervised anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee7672b-a728-487f-bf93-517f3e7df6f6",
   "metadata": {},
   "source": [
    "# Q4. What are the main categories of anomaly detection algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee2c8f8-79fe-44b3-aa67-02a62f4bf54f",
   "metadata": {},
   "source": [
    "1. **Statistical Methods**:\n",
    "   - **Parametric Methods**: Assume a distribution for the data and identify anomalies based on statistical deviations from this distribution (e.g., Gaussian distribution).\n",
    "   - **Non-parametric Methods**: Don't make assumptions about the data distribution and detect anomalies based on rank or order statistics (e.g., percentile-based methods).\n",
    "\n",
    "2. **Machine Learning Methods**:\n",
    "   - **Supervised Learning**: Utilize labeled data to train models to distinguish between normal and anomalous instances (e.g., classification algorithms like Support Vector Machines, Random Forests).\n",
    "   - **Unsupervised Learning**: Learn patterns from unlabeled data and detect deviations from normal behavior without prior knowledge of anomalies (e.g., clustering algorithms like k-means, density-based methods like DBSCAN, reconstruction-based methods like autoencoders).\n",
    "   - **Semi-supervised Learning**: Combine both labeled and unlabeled data during training to improve anomaly detection performance (e.g., using a small amount of labeled anomalies to augment unsupervised methods).\n",
    "\n",
    "3. **Proximity-based Methods**:\n",
    "   - **Distance-based Methods**: Measure the distance or similarity between data points and identify instances that are significantly different from their neighbors (e.g., k-nearest neighbors).\n",
    "   - **Density-based Methods**: Identify regions of low data density and flag instances that fall outside these regions as anomalies (e.g., DBSCAN).\n",
    "\n",
    "4. **Information Theory Methods**:\n",
    "   - Utilize concepts from information theory to quantify the unexpectedness or novelty of data instances (e.g., entropy-based methods).\n",
    "\n",
    "5. **Ensemble Methods**:\n",
    "   - Combine multiple anomaly detection algorithms to improve overall performance and robustness (e.g., using a combination of statistical, machine learning, and proximity-based methods)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92514086-2b52-4f54-9712-8c18e7f1cb1b",
   "metadata": {},
   "source": [
    "# Q5. What are the main assumptions made by distance-based anomaly detection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8076934-99da-4d86-bc8b-85bcf9354279",
   "metadata": {},
   "source": [
    "1. **Distance Metric**: These methods assume the availability of a meaningful distance or similarity metric that can effectively measure the similarity between data instances in the feature space. Common distance metrics include Euclidean distance, Manhattan distance, Mahalanobis distance, cosine similarity, etc.\n",
    "\n",
    "2. **Uniform Density**: They assume that normal instances are uniformly distributed in the feature space, meaning that there are no dense clusters of normal instances that would obscure the detection of anomalies.\n",
    "\n",
    "3. **Anomalies are Isolated**: Distance-based methods assume that anomalies are isolated instances that are significantly different from the majority of normal instances. This assumption implies that anomalies have a relatively large distance to their nearest neighbors.\n",
    "\n",
    "4. **Scalability**: These methods often assume that computing distances between data instances is computationally feasible, especially in high-dimensional spaces.\n",
    "\n",
    "5. **Linear Separability**: Some distance-based methods assume that anomalies can be effectively separated from normal instances using linear decision boundaries or distance thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d25d5a-ec7f-4f84-88be-4caaa48c0d48",
   "metadata": {},
   "source": [
    "# Q6. How does the LOF algorithm compute anomaly scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2089fe-77db-4f03-a56c-119f36de5549",
   "metadata": {},
   "source": [
    "The LOF (Local Outlier Factor) algorithm computes anomaly scores based on the local density deviation of a data point compared to its neighbors. Here's how it works:\n",
    "\n",
    "1. **Local Density Estimation**:\n",
    "   - For each data point \\( p \\), LOF computes its local density based on the distance to its \\( k \\) nearest neighbors. The local density of \\( p \\) is essentially the inverse of the average distance to its neighbors. Data points with higher local densities are considered to be part of dense regions, while those with lower densities are in sparse regions.\n",
    "\n",
    "2. **Local Reachability Density (LRD)**:\n",
    "   - LOF computes the local reachability density (LRD) for each data point by comparing its local density to the local densities of its neighbors. The LRD of a point \\( p \\) with respect to its neighbor \\( q \\) is the ratio of the average distance to \\( q \\) and the local density of \\( q \\). This measures how reachable \\( p \\) is from its neighbors in terms of local density.\n",
    "\n",
    "3. **Local Outlier Factor (LOF)**:\n",
    "   - Finally, the LOF of each data point \\( p \\) is computed as the average ratio of the LRD of \\( p \\) with respect to its neighbors to the LRD of its neighbors. Essentially, it compares the local reachability densities of \\( p \\) and its neighbors. Data points with significantly higher LOF scores than their neighbors are considered anomalies, as they have lower local densities compared to their surroundings, indicating that they are less well-connected to their local neighborhoods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0311eb73-f5b0-4529-b78d-a57f4ded763b",
   "metadata": {},
   "source": [
    "# Q7. What are the key parameters of the Isolation Forest algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e743e0e6-a7ec-4799-a1b2-b437e9b874a6",
   "metadata": {},
   "source": [
    "The Isolation Forest algorithm is a popular unsupervised anomaly detection algorithm that isolates anomalies by randomly partitioning the data space into isolation trees. Its key parameters include:\n",
    "\n",
    "1. **Number of Trees (n_estimators)**:\n",
    "   - Determines the number of isolation trees to build. Increasing the number of trees can improve the algorithm's performance but also increases computation time.\n",
    "\n",
    "2. **Subsample Size (max_samples)**:\n",
    "   - Specifies the number of samples to draw from the dataset to build each isolation tree. Smaller subsample sizes can speed up training but may reduce the algorithm's effectiveness in capturing the underlying data structure.\n",
    "\n",
    "3. **Maximum Tree Depth (max_depth)**:\n",
    "   - Controls the maximum depth of each isolation tree. Deeper trees can capture more complex patterns in the data but may also lead to overfitting, especially with smaller datasets.\n",
    "\n",
    "4. **Contamination**:\n",
    "   - Represents the expected proportion of anomalies in the dataset. It's used to adjust the decision threshold for identifying anomalies. Higher contamination values result in more anomalies being detected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc6f738-6035-4feb-b0e3-d17cbd459974",
   "metadata": {},
   "source": [
    "# Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score using KNN with K=10?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff65e74e-5752-4b4a-b63a-240cdbc3529c",
   "metadata": {},
   "source": [
    "To calculate the anomaly score using the k-nearest neighbors (KNN) algorithm, we need to consider the distance of the data point to its k-nearest neighbors and how many neighbors belong to the same class. \n",
    "\n",
    "In this case, the data point has only 2 neighbors of the same class within a radius of 0.5. We can calculate the anomaly score as follows:\n",
    "\n",
    "1. Since \\( K = 10 \\) and the data point has only 2 neighbors within a radius of 0.5, it doesn't satisfy the condition of having \\( K \\) neighbors. Therefore, we need to assign a penalty to the anomaly score.\n",
    "\n",
    "2. We can assign a higher anomaly score because the data point has only a small fraction of the required \\( K \\) neighbors within the specified radius, which indicates that it is an outlier compared to the majority of the points in its neighborhood.\n",
    "\n",
    "3. However, since both of its neighbors are of the same class, it suggests that the data point might not be an outlier in terms of its class distribution.\n",
    "\n",
    "4. Considering the limited information provided, we could assign a relatively high anomaly score to this data point to reflect its outlier status due to the lack of neighbors within the defined radius, but with a caveat that it might not be an anomaly in terms of its class distribution.\n",
    "\n",
    "So, without specific distance values or additional information about the dataset, it's challenging to provide an exact anomaly score. However, given the information provided, the anomaly score would likely be relatively high due to the scarcity of neighbors within the specified radius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0a809b5-ab34-4ac2-b5b4-ef9fd768e822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Score: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Assuming the data point has only 2 neighbors of the same class within a radius of 0.5\n",
    "num_neighbors_same_class = 2\n",
    "total_neighbors = 10  # Total number of neighbors considered (K=10)\n",
    "\n",
    "# Calculate the anomaly score based on the ratio of neighbors of the same class\n",
    "anomaly_score = 1 - (num_neighbors_same_class / total_neighbors)\n",
    "\n",
    "print(\"Anomaly Score:\", anomaly_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bc6376-4b55-499a-9078-d803b4d047b1",
   "metadata": {},
   "source": [
    "# Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the anomaly score for a data point that has an average path length of 5.0 compared to the average path length of the trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14da6a92-5ec7-4767-9e4b-17e0cdb7c12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Score: 0.9957711543647215\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given parameters\n",
    "num_trees = 100\n",
    "total_data_points = 3000\n",
    "avg_path_length_data_point = 5.0\n",
    "\n",
    "# Generate hypothetical average path lengths of trees in the training set\n",
    "avg_path_lengths_trees = np.random.normal(loc=10, scale=2, size=num_trees)  # Example distribution\n",
    "\n",
    "# Calculate the mean and standard deviation of the average path lengths of trees\n",
    "mean_avg_path_length_trees = np.mean(avg_path_lengths_trees)\n",
    "std_avg_path_length_trees = np.std(avg_path_lengths_trees)\n",
    "\n",
    "# Calculate z-score for the data point's average path length\n",
    "z_score = (avg_path_length_data_point - mean_avg_path_length_trees) / std_avg_path_length_trees\n",
    "\n",
    "# Calculate anomaly score using z-score\n",
    "anomaly_score = 1 - 0.5 * (1 + np.tanh(z_score))  # Sigmoid transformation to map z-score to [0, 1]\n",
    "\n",
    "print(\"Anomaly Score:\", anomaly_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a8ee38-ca1b-4a45-a3b4-d5beae483972",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
